{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "V5E1",
      "runtime_attributes": {
        "runtime_version": "2025.07"
      },
      "mount_file_id": "1pSkhc8MUO-ZuAIpl27JZSKUJiVX24vbj",
      "authorship_tag": "ABX9TyMGIR7hSJWY8C07RYa37Xyj",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/DHHirdal/MTech_AIML/blob/main/Group44_ProgrammingAssignment_TDML_TPU.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Note: Hw accelerator is v5e-1 TPU**\n",
        "\n",
        "Runtime â†’ Change runtime type â†’ Hardware accelerator: TPU."
      ],
      "metadata": {
        "id": "OBlSNBXqd2mR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Group Name  :  Group 44\n",
        "#Members\n",
        "  \n",
        "Name\t                                BITS ID\n",
        "  - CHELLA VENKATA GOPIKRISHNA\t  -->       2024ac05978\n",
        "  - HIRDALAPPA H\t     --->                2024ac05306\t    \n",
        "  - ALOK KUMAR OJHA\t-->                2024ad05055\n",
        "  - PUVVADA VENKATA SAI MANOJ CHANDRA --> 2024ac05227\n",
        "  - SK SAFIRUDDIN\t     --->                 2024ac05781\n"
      ],
      "metadata": {
        "id": "aGYBtdzKTVkj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Our techical paper has two parts**\n",
        "\n",
        "*   **Part-1:** Explore Distributed Machine learning using PyTorch DDP\n",
        "*   **Part -2:** Explore the TDML framework (Trustworthy Distributed Machine Learning ) like Blockchain-based Data Parallelism"
      ],
      "metadata": {
        "id": "mX2iT9W-TVKh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Install required libraries\n",
        "\n",
        "%pip install torch"
      ],
      "metadata": {
        "id": "KFn6lCnXZJXj",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Import required libraries\n",
        "import math\n",
        "import random\n",
        "import copy\n",
        "import time\n",
        "import numpy as np\n",
        "from typing import List, Tuple, Dict, Any, Optional\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "import sys\n",
        "import platform\n",
        "import subprocess\n",
        "from datetime import datetime\n",
        "from collections import defaultdict\n",
        "import random\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torchvision import models, datasets, transforms\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torchvision import models, datasets, transforms\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "#PyTorch Libraries\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import torchvision\n",
        "from torch.utils.data import DataLoader, Subset, random_split\n",
        "from torchvision import datasets, transforms, models\n",
        "from torch.utils.tensorboard import SummaryWriter"
      ],
      "metadata": {
        "id": "TAHm1K19ZpLd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Part-1:** Explore Distributed Machine learning using PyTorch DDP"
      ],
      "metadata": {
        "id": "1c3HXl4sULA6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "metadata": {
        "id": "ntIuEvaQbJie"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## System Parameters\n",
        "What PyTorch detects\n",
        "*   CPUs (cores, threads)\n",
        "*   GPUs (CUDA / ROCm)\n",
        "*   GPU memory\n",
        "*   NUMA topology (indirectly via OS)\n",
        "*   Accelerators (XPU, MPS, etc.)\n",
        "*   CPU core count\n",
        "*   Uses system RAM\n",
        "*   PyTorch uses multiple thread pools.\n",
        "\n",
        "\n",
        "\n",
        "Distributed Training: Systemâ€‘Level Design\n",
        "**PyTorch Distributed (torch.distributed)** is explicitly systemâ€‘aware.\n",
        "**In PyTorch,** system details directly shape execution speed, memory usage, scalability, and reliabilityâ€”especially in distributed training.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "4iExVoSB-jqE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "ðŸ“Œ PyTorch choice: torch.nn.parallel.DistributedDataParallel (DDP)"
      ],
      "metadata": {
        "id": "10Ku8xNsy5le"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Optional dependency â€” used if available\n",
        "try:\n",
        "    import psutil\n",
        "except Exception:\n",
        "    psutil = None\n",
        "\n",
        "import torch\n",
        "\n",
        "def _bytes2human(n: int) -> str:\n",
        "    symbols = ('B','KB','MB','GB','TB','PB')\n",
        "    i = 0\n",
        "    n = float(n)\n",
        "    while n >= 1024 and i < len(symbols) - 1:\n",
        "        n /= 1024.0\n",
        "        i += 1\n",
        "    return f\"{n:.2f} {symbols[i]}\"\n",
        "\n",
        "def _safe_run(cmd: str) -> str | None:\n",
        "    try:\n",
        "        return subprocess.check_output(\n",
        "            cmd, shell=True, stderr=subprocess.DEVNULL, text=True\n",
        "        ).strip()\n",
        "    except Exception:\n",
        "        return None\n",
        "\n",
        "def print_basic_runtime():\n",
        "    print(\"â•\" * 90)\n",
        "    print(\"System / Runtime Summary\")\n",
        "    print(\"Time:\", datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"))\n",
        "    print(\"Python:\", sys.version.split()[0])\n",
        "    print(\"OS:\", platform.platform())\n",
        "    print(\"Machine:\", platform.machine(), \"| Processor:\", platform.processor() or \"N/A\")\n",
        "    print(\"CPU cores (logical):\", os.cpu_count())\n",
        "\n",
        "    # CPU frequency (if available)\n",
        "    if psutil and hasattr(psutil, \"cpu_freq\"):\n",
        "        try:\n",
        "            f = psutil.cpu_freq()\n",
        "            if f:\n",
        "                print(f\"CPU freq: current {f.current:.0f} MHz | min {f.min:.0f} | max {f.max:.0f}\")\n",
        "        except Exception:\n",
        "            pass\n",
        "\n",
        "    # Memory\n",
        "    if psutil:\n",
        "        try:\n",
        "            vm = psutil.virtual_memory()\n",
        "            print(f\"RAM total: {_bytes2human(vm.total)} | available: {_bytes2human(vm.available)} \"\n",
        "                  f\"| used: {_bytes2human(vm.used)} ({vm.percent}%)\")\n",
        "        except Exception:\n",
        "            pass\n",
        "    else:\n",
        "        print(\"RAM: psutil not available (install `psutil` for detailed memory stats)\")\n",
        "\n",
        "    # Disk\n",
        "    if psutil:\n",
        "        try:\n",
        "            d = psutil.disk_usage(\"/\")\n",
        "            print(f\"Disk '/': total {_bytes2human(d.total)} | used {_bytes2human(d.used)} \"\n",
        "                  f\"({d.percent}%) | free {_bytes2human(d.free)}\")\n",
        "        except Exception:\n",
        "            pass\n",
        "\n",
        "    # PyTorch\n",
        "    print(\"-\" * 90)\n",
        "    print(\"PyTorch:\", torch.__version__)\n",
        "    print(\"CUDA available:\", torch.cuda.is_available())\n",
        "    print(\"CUDA (pytorch build):\", torch.version.cuda)\n",
        "    # cuDNN info\n",
        "    try:\n",
        "        print(\"cuDNN enabled:\", torch.backends.cudnn.enabled, \"| version:\", torch.backends.cudnn.version())\n",
        "    except Exception:\n",
        "        pass\n",
        "\n",
        "    # NVIDIA-SMI, if present\n",
        "    nvidia_smi = _safe_run(\"nvidia-smi --query-gpu=name,driver_version,memory.total --format=csv,noheader\")\n",
        "    if nvidia_smi:\n",
        "        print(\"nvidia-smi:\", nvidia_smi)\n",
        "\n",
        "    # GPU details\n",
        "    if torch.cuda.is_available():\n",
        "        try:\n",
        "            num = torch.cuda.device_count()\n",
        "            print(\"CUDA device count:\", num)\n",
        "            for i in range(num):\n",
        "                prop = torch.cuda.get_device_properties(i)\n",
        "                total_mem_gb = prop.total_memory / (1024**3)\n",
        "                print(f\"  [{i}] {prop.name} | CC {prop.major}.{prop.minor} | VRAM: {total_mem_gb:.1f} GB\")\n",
        "            try:\n",
        "                cur = torch.cuda.current_device()\n",
        "                print(\"Active device:\", cur, \"-\", torch.cuda.get_device_name(cur))\n",
        "            except Exception:\n",
        "                pass\n",
        "        except Exception as e:\n",
        "            print(\"CUDA detail error:\", str(e))\n",
        "\n",
        "    # Apple MPS\n",
        "    has_mps = hasattr(torch.backends, \"mps\") and torch.backends.mps.is_available()\n",
        "    print(\"MPS available:\", has_mps)\n",
        "    if has_mps:\n",
        "        print(\"MPS build supported:\", torch.backends.mps.is_built())\n",
        "\n",
        "    # AMP availability (CUDA)\n",
        "    try:\n",
        "        use_cuda = torch.cuda.is_available\n",
        "        amp_enabled = bool(use_cuda)  # Gate AMP on CUDA availability\n",
        "    except Exception:\n",
        "        amp_enabled = False\n",
        "    print(\"AMP (CUDA) available:\", bool(amp_enabled))\n",
        "\n",
        "    # Current process memory\n",
        "    if psutil:\n",
        "        try:\n",
        "            proc = psutil.Process(os.getpid())\n",
        "            mem = proc.memory_info().rss\n",
        "            print(f\"Current process RSS: {_bytes2human(mem)}\")\n",
        "        except Exception:\n",
        "            pass\n",
        "\n",
        "    print(\"â•\" * 90)\n",
        "\n",
        "def print_distributed_readiness():\n",
        "    print(\"Distributed / DDP Readiness\")\n",
        "    # Backends\n",
        "    dist_avail = torch.distributed.is_available()\n",
        "    print(\"torch.distributed available:\", dist_avail)\n",
        "    if dist_avail:\n",
        "        try:\n",
        "            print(\"  - NCCL available:\", torch.distributed.is_nccl_available())\n",
        "        except Exception:\n",
        "            print(\"  - NCCL available: (unknown)\")\n",
        "\n",
        "        try:\n",
        "            print(\"  - Gloo available:\", torch.distributed.is_gloo_available())\n",
        "        except Exception:\n",
        "            print(\"  - Gloo available: (unknown)\")\n",
        "\n",
        "        try:\n",
        "            print(\"  - MPI available:\", torch.distributed.is_mpi_available())\n",
        "        except Exception:\n",
        "            print(\"  - MPI available: (unknown)\")\n",
        "\n",
        "    # Env vars commonly relevant to DDP/NCCL\n",
        "    env_keys = [\n",
        "        \"CUDA_VISIBLE_DEVICES\",\n",
        "        \"NCCL_DEBUG\",\n",
        "        \"NCCL_SOCKET_IFNAME\",\n",
        "        \"NCCL_IB_DISABLE\",\n",
        "        \"NCCL_P2P_DISABLE\",\n",
        "        \"NCCL_SHM_DISABLE\",\n",
        "        \"MASTER_ADDR\",\n",
        "        \"MASTER_PORT\",\n",
        "        \"WORLD_SIZE\",\n",
        "        \"RANK\",\n",
        "        \"LOCAL_RANK\",\n",
        "    ]\n",
        "    print(\"Important environment variables:\")\n",
        "    for k in env_keys:\n",
        "        v = os.environ.get(k, None)\n",
        "        print(f\"  {k} = {v}\")\n",
        "\n",
        "    # Peer-to-peer (P2P) accessibility among GPUs (intra-node)\n",
        "    if torch.cuda.is_available():\n",
        "        n = torch.cuda.device_count()\n",
        "        if n > 1:\n",
        "            print(\"GPU Peer-to-Peer access matrix (device_can_access_peer):\")\n",
        "            for i in range(n):\n",
        "                row = []\n",
        "                for j in range(n):\n",
        "                    if i == j:\n",
        "                        row.append(\"â€”\")\n",
        "                    else:\n",
        "                        try:\n",
        "                            row.append(\"Y\" if torch.cuda.device_can_access_peer(i, j) else \"N\")\n",
        "                        except Exception:\n",
        "                            row.append(\"?\")\n",
        "                print(f\"  {i}: \" + \" \".join(row))\n",
        "        else:\n",
        "            print(\"Single CUDA device detected; P2P not applicable.\")\n",
        "    else:\n",
        "        print(\"CUDA not available; NCCL/P2P not applicable.\")\n",
        "\n",
        "    # Quick single-process init test (optional and safe on single node)\n",
        "    # Attempts to create/destroy a process group (Gloo on CPU; NCCL on GPU)\n",
        "    if dist_avail:\n",
        "        backend = \"nccl\" if torch.cuda.is_available() else \"gloo\"\n",
        "        try:\n",
        "            if not torch.distributed.is_initialized():\n",
        "                torch.distributed.init_process_group(\n",
        "                    backend=backend,\n",
        "                    rank=0,\n",
        "                    world_size=1,\n",
        "                    init_method=\"env://\"\n",
        "                )\n",
        "            print(f\"Process group init test: OK (backend={backend})\")\n",
        "        except Exception as e:\n",
        "            print(f\"Process group init test: FAILED (backend={backend}) -> {e}\")\n",
        "        finally:\n",
        "            try:\n",
        "                if torch.distributed.is_initialized():\n",
        "                    torch.distributed.destroy_process_group()\n",
        "            except Exception:\n",
        "                pass\n",
        "\n",
        "    # Report NCCL runtime version if available via nvidia-smi / nvcc\n",
        "    nvcc = _safe_run(\"nvcc --version\")\n",
        "    if nvcc:\n",
        "        print(\"nvcc --version:\")\n",
        "        for line in nvcc.splitlines():\n",
        "            print(\"  \" + line)\n",
        "\n",
        "    # Optional: show nvidia-smi topo for links/NVLink (if installed)\n",
        "    topo = _safe_run(\"nvidia-smi topo -m\")\n",
        "    if topo:\n",
        "        print(\"nvidia-smi topo -m:\")\n",
        "        for line in topo.splitlines():\n",
        "            print(\"  \" + line)\n",
        "\n",
        "    print(\"â•\" * 90)\n",
        "\n",
        "def main():\n",
        "    print(\"Python:\", platform.python_version())\n",
        "    print(\"PyTorch:\", torch.__version__)\n",
        "    print(\"CUDA available:\", torch.cuda.is_available())\n",
        "    if torch.cuda.is_available():\n",
        "        print(\"GPU name:\", torch.cuda.get_device_name(0))\n",
        "        print(\"CUDA version (pytorch build):\", torch.version.cuda)\n",
        "        print(\"#GPUs visible:\", torch.cuda.device_count())\n",
        "        print(\"CUDA device name:\", torch.cuda.get_device_name(0))\n",
        "        print(\"CUDA device memory:\", torch.cuda.get_device_properties(0).total_memory)\n",
        "\n",
        "    # Full summaries\n",
        "    print_basic_runtime()\n",
        "    print_distributed_readiness()\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "id": "Lg_ZS_-pEywq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "============================================================================"
      ],
      "metadata": {
        "id": "yPm598VEJmA8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##TDML framework using the ResNet50 model on the CIFAR-10 dataset\n",
        " # DNN training parallelism\n",
        "\n",
        "This experiment aim to demonstrate that TDML framework can match the performance of single-node training, achieve comparable accuracy to traditional methods despite the challenges of FL, and enhance efficiency in terms of convergence speeds and training loss."
      ],
      "metadata": {
        "id": "_048Z6OtJoqo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "ðŸ“Œ PyTorch choice: torch.nn.parallel.DistributedDataParallel (DDP)"
      ],
      "metadata": {
        "id": "ETM2SaEZx4rL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"[INFO] Dataset loading and confirmation\")\n",
        "\n",
        "class ResNet50CIFAR(nn.Module):\n",
        "  def __init__(self) -> None:\n",
        "    super().__init__()\n",
        "    base = models.resnet50(weights=None)\n",
        "    in_features = base.fc.in_features\n",
        "    base.fc = nn.Linear(in_features, 10)\n",
        "    self.model = base\n",
        "\n",
        "  def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        return self.model(x)\n",
        "\n",
        "# Define a basic transform to convert PIL Image to tensor\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor()\n",
        "])\n",
        "\n",
        "train_ds = datasets.CIFAR10(root=\"./data\", train=True, download=True, transform=transform)\n",
        "test_ds = datasets.CIFAR10(root=\"./data\", train=False, download=True, transform=transform)\n",
        "\n",
        "\n",
        "print(\"Dataset length:\", len(train_ds))                 # 50000\n",
        "print(\"Classes:\", train_ds.classes)                     # ['airplane', 'automobile', ...]\n",
        "print(\"Number of classes:\", len(train_ds.classes))      # 10\n",
        "print(\"Image shape:\", train_ds[0][0].shape)\n",
        "\n",
        "\n",
        "print(\"Dataset loaded successfully and verified\")"
      ],
      "metadata": {
        "id": "IAQbkmqQcUyT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ------------------------------\n",
        "# Data: sharded train loaders + single test loader\n",
        "# ------------------------------\n",
        "def get_data(num_workers: int = 5, batch_size: int = 64):\n",
        "    \"\"\"\n",
        "    Returns:\n",
        "        worker_loaders: list[DataLoader]  # one training DataLoader per shard\n",
        "        test_loader: DataLoader           # test loader\n",
        "        classes: list[str]                # CIFAR-10 class names\n",
        "        mean, std: tuple[float]           # used for Normalize (for de-normalization)\n",
        "    Notes:\n",
        "        - Here `num_workers` is treated as the number of shards for train set (to match your signature).\n",
        "        - We use a small fixed number of loader worker processes per DataLoader (loader_workers=2).\n",
        "    \"\"\"\n",
        "    # 1) Define transforms (CIFAR-10 is 32x32 RGB)\n",
        "    mean = (0.4914, 0.4822, 0.4465)\n",
        "    std  = (0.2470, 0.2435, 0.2616)  # Keep this consistent everywhere (train/test/denorm)\n",
        "\n",
        "    transform = transforms.Compose([\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(mean=mean, std=std),\n",
        "    ])\n",
        "\n",
        "    # 2) Create Dataset objects\n",
        "    train_ds = datasets.CIFAR10(root=\"./data\", train=True,  download=True, transform=transform)\n",
        "    test_ds  = datasets.CIFAR10(root=\"./data\", train=False, download=True, transform=transform)\n",
        "\n",
        "    # 3) Split training set into shards and create loaders\n",
        "    num_shards = max(1, int(num_workers))  # reuse your parameter name as \"number of shards\"\n",
        "    n = len(train_ds)                      # 50_000 for CIFAR-10\n",
        "    shard_size = n // num_shards\n",
        "    worker_loaders: List[DataLoader] = []\n",
        "\n",
        "    # Keep DataLoader worker processes reasonable (Colab often does fine with 2)\n",
        "    loader_workers = 2\n",
        "    pin = torch.cuda.is_available()\n",
        "\n",
        "    for i in range(num_shards):\n",
        "        start = i * shard_size\n",
        "        end = (i + 1) * shard_size if i < num_shards - 1 else n  # last shard gets the remainder\n",
        "\n",
        "        subset = Subset(train_ds, range(start, end))\n",
        "        loader = DataLoader(\n",
        "            subset,\n",
        "            batch_size=batch_size,\n",
        "            shuffle=True,\n",
        "            num_workers=loader_workers,\n",
        "            pin_memory=pin,\n",
        "            persistent_workers=(loader_workers > 0),\n",
        "            drop_last=False,\n",
        "        )\n",
        "        worker_loaders.append(loader)\n",
        "\n",
        "    # 4) Single test loader (create once, outside the loop)\n",
        "    test_loader = DataLoader(\n",
        "        test_ds,\n",
        "        batch_size=batch_size,\n",
        "        shuffle=False,\n",
        "        num_workers=loader_workers,\n",
        "        pin_memory=pin,\n",
        "        persistent_workers=(loader_workers > 0),\n",
        "        drop_last=False,\n",
        "    )\n",
        "\n",
        "    return worker_loaders, test_loader, list(train_ds.classes), mean, std\n",
        "\n",
        "\n",
        "# ------------------------------\n",
        "# Helpers: de-normalization (batch-safe)\n",
        "# ------------------------------\n",
        "def _as_batch_stats_tensors(mean, std, device=\"cpu\"):\n",
        "    mean_t = torch.tensor(mean, dtype=torch.float32, device=device).view(1, 3, 1, 1)\n",
        "    std_t  = torch.tensor(std,  dtype=torch.float32, device=device).view(1, 3, 1, 1)\n",
        "    return mean_t, std_t\n",
        "\n",
        "def denorm(batch, mean, std):\n",
        "    \"\"\"\n",
        "    De-normalize a batch of images [N,3,H,W].\n",
        "    \"\"\"\n",
        "    device = batch.device\n",
        "    mean_t, std_t = _as_batch_stats_tensors(mean, std, device=device)\n",
        "    return batch * std_t + mean_t\n",
        "\n",
        "\n",
        "# ------------------------------\n",
        "# Visualization 1: Predictions vs Labels grid\n",
        "# ------------------------------\n",
        "def show_preds_vs_labels(model, loader, classes, mean, std, device=\"cuda\", max_images=16, nrow=4):\n",
        "    \"\"\"\n",
        "    Shows a grid of images with titles \"Pred: <pred> / True: <true>\".\n",
        "    Misclassifications are shown in red; correct in green.\n",
        "    \"\"\"\n",
        "    device = device if (device == \"cuda\" and torch.cuda.is_available()) else \"cpu\"\n",
        "    model.to(device)\n",
        "    model.eval()\n",
        "\n",
        "    with torch.no_grad():\n",
        "        images, labels = next(iter(loader))\n",
        "        images = images.to(device)\n",
        "        labels = labels.to(device)\n",
        "\n",
        "        logits = model(images)\n",
        "        preds = logits.argmax(dim=1)\n",
        "\n",
        "        # Keep only first max_images\n",
        "        images = images[:max_images].cpu()\n",
        "        labels = labels[:max_images].cpu()\n",
        "        preds  = preds[:max_images].cpu()\n",
        "\n",
        "    # De-normalize for display\n",
        "    disp = denorm(images, mean, std).clamp(0, 1)\n",
        "\n",
        "    rows = int(np.ceil(max_images / nrow))\n",
        "    fig, axes = plt.subplots(rows, nrow, figsize=(nrow * 3.0, rows * 3.0))\n",
        "    axes = np.array(axes).reshape(-1)  # flatten\n",
        "\n",
        "    for i in range(rows * nrow):\n",
        "        ax = axes[i]\n",
        "        if i < len(disp):\n",
        "            img = disp[i].permute(1, 2, 0).numpy()\n",
        "            true_lbl = classes[labels[i].item()]\n",
        "            pred_lbl = classes[preds[i].item()]\n",
        "            color = \"green\" if pred_lbl == true_lbl else \"red\"\n",
        "\n",
        "            ax.imshow(img)\n",
        "            ax.set_title(f\"Pred: {pred_lbl}\\nTrue: {true_lbl}\", color=color, fontsize=9)\n",
        "            ax.axis(\"off\")\n",
        "        else:\n",
        "            ax.axis(\"off\")\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "# ------------------------------\n",
        "# Visualization 2: Confusion Matrix\n",
        "# ------------------------------\n",
        "def plot_confusion_matrix(model, loader, classes, device=\"cuda\", normalize=False, cmap=\"Blues\"):\n",
        "    \"\"\"\n",
        "    Computes and plots a confusion matrix across the entire loader.\n",
        "    Works without scikit-learn.\n",
        "    \"\"\"\n",
        "    device = device if (device == \"cuda\" and torch.cuda.is_available()) else \"cpu\"\n",
        "    model.to(device)\n",
        "    model.eval()\n",
        "\n",
        "    num_classes = len(classes)\n",
        "    cm = torch.zeros((num_classes, num_classes), dtype=torch.int64)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for x, y in loader:\n",
        "            x = x.to(device, non_blocking=True)\n",
        "            y = y.to(device, non_blocking=True)\n",
        "            preds = model(x).argmax(dim=1)\n",
        "            for t, p in zip(y.view(-1), preds.view(-1)):\n",
        "                cm[t.long(), p.long()] += 1\n",
        "\n",
        "    cm_np = cm.numpy().astype(np.float64)\n",
        "    if normalize:\n",
        "        row_sums = cm_np.sum(axis=1, keepdims=True)\n",
        "        row_sums[row_sums == 0] = 1\n",
        "        cm_np = cm_np / row_sums\n",
        "\n",
        "    fig, ax = plt.subplots(figsize=(7, 6))\n",
        "    im = ax.imshow(cm_np, interpolation=\"nearest\", cmap=cmap)\n",
        "    plt.colorbar(im, ax=ax, fraction=0.046, pad=0.04)\n",
        "\n",
        "    ax.set(\n",
        "        xticks=np.arange(num_classes),\n",
        "        yticks=np.arange(num_classes),\n",
        "        xticklabels=classes,\n",
        "        yticklabels=classes,\n",
        "        ylabel=\"True label\",\n",
        "        xlabel=\"Predicted label\",\n",
        "        title=\"Confusion Matrix (normalized)\" if normalize else \"Confusion Matrix\",\n",
        "    )\n",
        "    plt.setp(ax.get_xticklabels(), rotation=45, ha=\"right\", rotation_mode=\"anchor\")\n",
        "\n",
        "    # annotate cells\n",
        "    fmt = \".2f\" if normalize else \"d\"\n",
        "    thresh = cm_np.max() / 2.0\n",
        "    for i in range(num_classes):\n",
        "        for j in range(num_classes):\n",
        "            ax.text(j, i, format(cm_np[i, j], fmt),\n",
        "                    ha=\"center\", va=\"center\",\n",
        "                    color=\"white\" if cm_np[i, j] > thresh else \"black\")\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "JkDxln6ict4F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "=============================================================="
      ],
      "metadata": {
        "id": "rrgy12JNch0g"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Train the CNN Model - using worker and batch_size\n",
        "\n",
        "- define the  Orchestrate, Training & model Evaluation\n",
        "- Visualize the Loss & Accuracy Curves, Confusion matrix"
      ],
      "metadata": {
        "id": "ssr_jKEVcj0Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# =========================================\n",
        "# CIFAR-10: Data, Training, Plots (One-Cell)\n",
        "# =========================================\n",
        "\n",
        "# ------------------------------\n",
        "# Data: sharded train loaders + single test loader\n",
        "# ------------------------------\n",
        "\n",
        "import time\n",
        "def get_data(num_workers: int = 4, batch_size: int = 128):\n",
        "    \"\"\"\n",
        "    Returns:\n",
        "        worker_loaders: list[DataLoader]  # one training DataLoader per shard\n",
        "        test_loader: DataLoader           # test loader\n",
        "        classes: list[str]                # CIFAR-10 class names\n",
        "        mean, std: tuple[float]           # used for Normalize (for de-normalization)\n",
        "\n",
        "    Note:\n",
        "        - Here `num_workers` is treated as the number of *shards* for the train set\n",
        "          (to stay compatible with your earlier signature).\n",
        "        - DataLoader subprocesses per loader are set to a small fixed value (loader_workers=2).\n",
        "    \"\"\"\n",
        "    # 1) Define transforms (CIFAR-10 is 32x32 RGB)\n",
        "    mean = (0.4914, 0.4822, 0.4465)\n",
        "    std  = (0.2470, 0.2435, 0.2616)\n",
        "\n",
        "    transform_train = transforms.Compose([\n",
        "        transforms.RandomCrop(32, padding=4),\n",
        "        transforms.RandomHorizontalFlip(),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(mean=mean, std=std),\n",
        "    ])\n",
        "    transform_test = transforms.Compose([\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(mean=mean, std=std),\n",
        "    ])\n",
        "\n",
        "    # 2) Create Dataset objects\n",
        "    train_ds = datasets.CIFAR10(root=\"./data\", train=True,  download=True, transform=transform_train)\n",
        "    test_ds  = datasets.CIFAR10(root=\"./data\", train=False, download=True, transform=transform_test)\n",
        "\n",
        "    # 3) Split training set into shards and create loaders\n",
        "    num_shards = max(1, int(num_workers))  # reusing your parameter as number of shards\n",
        "    n = len(train_ds)                      # 50_000 for CIFAR-10\n",
        "    shard_size = n // num_shards\n",
        "    worker_loaders: List[DataLoader] = []\n",
        "\n",
        "    loader_workers = 2  # per-DataLoader subprocesses (Colab-friendly)\n",
        "    pin = torch.cuda.is_available()\n",
        "\n",
        "    for i in range(num_shards):\n",
        "        start = i * shard_size\n",
        "        end = (i + 1) * shard_size if i < num_shards - 1 else n  # last shard gets the remainder\n",
        "\n",
        "        subset = Subset(train_ds, range(start, end))\n",
        "        loader = DataLoader(\n",
        "            subset,\n",
        "            batch_size=batch_size,\n",
        "            shuffle=True,\n",
        "            num_workers=loader_workers,\n",
        "            pin_memory=pin,\n",
        "            persistent_workers=(loader_workers > 0),\n",
        "            drop_last=False,\n",
        "        )\n",
        "        worker_loaders.append(loader)\n",
        "\n",
        "    # 4) Single test loader (create once)\n",
        "    test_loader = DataLoader(\n",
        "        test_ds,\n",
        "        batch_size=batch_size * 2,\n",
        "        shuffle=False,\n",
        "        num_workers=loader_workers,\n",
        "        pin_memory=pin,\n",
        "        persistent_workers=(loader_workers > 0),\n",
        "        drop_last=False,\n",
        "    )\n",
        "\n",
        "    return worker_loaders, test_loader, list(train_ds.classes), mean, std\n",
        "\n",
        "\n",
        "# ------------------------------\n",
        "# Helpers: de-normalization (batch-safe)\n",
        "# ------------------------------\n",
        "def _as_batch_stats_tensors(mean, std, device=\"cpu\"):\n",
        "    mean_t = torch.tensor(mean, dtype=torch.float32, device=device).view(1, 3, 1, 1)\n",
        "    std_t  = torch.tensor(std,  dtype=torch.float32, device=device).view(1, 3, 1, 1)\n",
        "    return mean_t, std_t\n",
        "\n",
        "def denorm(batch, mean, std):\n",
        "    \"\"\"De-normalize a batch of images [N,3,H,W].\"\"\"\n",
        "    device = batch.device\n",
        "    mean_t, std_t = _as_batch_stats_tensors(mean, std, device=device)\n",
        "    return batch * std_t + mean_t\n",
        "\n",
        "\n",
        "# ------------------------------\n",
        "# Training & Evaluation + History\n",
        "# ------------------------------\n",
        "def accuracy_from_logits(logits, targets):\n",
        "    preds = logits.argmax(dim=1)\n",
        "    return (preds == targets).float().mean().item()\n",
        "\n",
        "@torch.no_grad()\n",
        "def evaluate_loop(model, loader, criterion, device=\"cuda\"):\n",
        "    model.eval()\n",
        "    device = device if (device == \"cuda\" and torch.cuda.is_available()) else \"cpu\"\n",
        "\n",
        "    total_loss = 0.0\n",
        "    total_correct = 0\n",
        "    total_samples = 0\n",
        "\n",
        "    for x, y in loader:\n",
        "        x = x.to(device, non_blocking=True)\n",
        "        y = y.to(device, non_blocking=True)\n",
        "\n",
        "        logits = model(x)\n",
        "        loss = criterion(logits, y)\n",
        "\n",
        "        total_loss += loss.item() * y.size(0)\n",
        "        total_correct += (logits.argmax(1) == y).sum().item()\n",
        "        total_samples += y.size(0)\n",
        "\n",
        "    avg_loss = total_loss / max(1, total_samples)\n",
        "    avg_acc  = total_correct / max(1, total_samples)\n",
        "    return avg_loss, avg_acc\n",
        "\n",
        "def train_with_history(\n",
        "    model,\n",
        "    train_loader: DataLoader,\n",
        "    val_loader: DataLoader,\n",
        "    epochs: int = 5,\n",
        "    lr: float = 3e-4,\n",
        "    weight_decay: float = 1e-4,\n",
        "    device: str = \"cuda\",\n",
        "    print_every: int = 1,\n",
        "):\n",
        "    \"\"\"\n",
        "    Train model and collect per-epoch metrics: train_loss, train_acc, val_loss, val_acc.\n",
        "    Returns:\n",
        "        history: dict with keys 'train_loss', 'train_acc', 'val_loss', 'val_acc'\n",
        "        model: trained model (on device)\n",
        "    \"\"\"\n",
        "    device = device if (device == \"cuda\" and torch.cuda.is_available()) else \"cpu\"\n",
        "    model.to(device)\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = optim.AdamW(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
        "\n",
        "    # (Optional) small perf tweak for fixed-size inputs\n",
        "    torch.backends.cudnn.benchmark = True\n",
        "\n",
        "    history = defaultdict(list)\n",
        "    t0 = time.time()\n",
        "\n",
        "    for epoch in range(1, epochs + 1):\n",
        "        model.train()\n",
        "        running_loss = 0.0\n",
        "        running_correct = 0\n",
        "        running_samples = 0\n",
        "\n",
        "        for x, y in train_loader:\n",
        "            x = x.to(device, non_blocking=True)\n",
        "            y = y.to(device, non_blocking=True)\n",
        "\n",
        "            optimizer.zero_grad(set_to_none=True)\n",
        "            logits = model(x)\n",
        "            loss = criterion(logits, y)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            running_loss += loss.item() * y.size(0)\n",
        "            running_correct += (logits.argmax(1) == y).sum().item()\n",
        "            running_samples += y.size(0)\n",
        "\n",
        "        train_loss = running_loss / max(1, running_samples)\n",
        "        train_acc  = running_correct / max(1, running_samples)\n",
        "\n",
        "        # Validation\n",
        "        val_loss, val_acc = evaluate_loop(model, val_loader, criterion, device=device)\n",
        "\n",
        "        # Record history\n",
        "        history[\"train_loss\"].append(train_loss)\n",
        "        history[\"train_acc\"].append(train_acc)\n",
        "        history[\"val_loss\"].append(val_loss)\n",
        "        history[\"val_acc\"].append(val_acc)\n",
        "\n",
        "        if epoch % print_every == 0:\n",
        "            print(f\"Epoch {epoch:02d} | \"\n",
        "                  f\"train_loss={train_loss:.4f} acc={train_acc:.3f} | \"\n",
        "                  f\"val_loss={val_loss:.4f} acc={val_acc:.3f}\")\n",
        "\n",
        "    t1 = time.time()\n",
        "    print(f\"Training finished in {(t1 - t0):.1f}s for {epochs} epochs.\")\n",
        "    return dict(history), model\n",
        "\n",
        "\n",
        "# ------------------------------\n",
        "# Plotting: Loss & Accuracy Curves\n",
        "# ------------------------------\n",
        "def plot_training_curves(history, title_prefix=\"CIFAR-10\"):\n",
        "    \"\"\"\n",
        "    Plot loss and accuracy curves from history dict with keys:\n",
        "    'train_loss', 'val_loss', 'train_acc', 'val_acc'\n",
        "    \"\"\"\n",
        "    epochs = np.arange(1, len(history[\"train_loss\"]) + 1)\n",
        "\n",
        "    plt.figure(figsize=(12, 4))\n",
        "\n",
        "    # Loss\n",
        "    plt.subplot(1, 2, 1)\n",
        "    plt.plot(epochs, history[\"train_loss\"], label=\"Train Loss\", marker=\"o\")\n",
        "    plt.plot(epochs, history[\"val_loss\"],   label=\"Val Loss\",   marker=\"o\")\n",
        "    plt.xlabel(\"Epoch\")\n",
        "    plt.ylabel(\"Loss\")\n",
        "    plt.title(f\"{title_prefix} â€” Loss\")\n",
        "    plt.grid(True, alpha=0.3)\n",
        "    plt.legend()\n",
        "\n",
        "    # Accuracy\n",
        "    plt.subplot(1, 2, 2)\n",
        "    plt.plot(epochs, np.array(history[\"train_acc\"]) * 100, label=\"Train Acc (%)\", marker=\"o\")\n",
        "    plt.plot(epochs, np.array(history[\"val_acc\"])   * 100, label=\"Val Acc (%)\",   marker=\"o\")\n",
        "    plt.xlabel(\"Epoch\")\n",
        "    plt.ylabel(\"Accuracy (%)\")\n",
        "    plt.title(f\"{title_prefix} â€” Accuracy\")\n",
        "    plt.grid(True, alpha=0.3)\n",
        "    plt.legend()\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "# ------------------------------\n",
        "# Visualization: Predictions vs Labels\n",
        "# ------------------------------\n",
        "def show_preds_vs_labels(model, loader, classes, mean, std, device=\"cuda\", max_images=16, nrow=4):\n",
        "    \"\"\"\n",
        "    Shows a grid of images with titles \"Pred: <pred> / True: <true>\".\n",
        "    Misclassifications are shown in red; correct in green.\n",
        "    \"\"\"\n",
        "    device = device if (device == \"cuda\" and torch.cuda.is_available()) else \"cpu\"\n",
        "    model.to(device)\n",
        "    model.eval()\n",
        "\n",
        "    with torch.no_grad():\n",
        "        images, labels = next(iter(loader))\n",
        "        images = images.to(device)\n",
        "        labels = labels.to(device)\n",
        "\n",
        "        logits = model(images)\n",
        "        preds = logits.argmax(dim=1)\n",
        "\n",
        "        # Keep only first max_images\n",
        "        images = images[:max_images].cpu()\n",
        "        labels = labels[:max_images].cpu()\n",
        "        preds  = preds[:max_images].cpu()\n",
        "\n",
        "    # De-normalize for display\n",
        "    disp = denorm(images, mean, std).clamp(0, 1)\n",
        "\n",
        "    rows = int(np.ceil(max_images / nrow))\n",
        "    fig, axes = plt.subplots(rows, nrow, figsize=(nrow * 3.0, rows * 3.0))\n",
        "    axes = np.array(axes).reshape(-1)  # flatten\n",
        "\n",
        "    for i in range(rows * nrow):\n",
        "        ax = axes[i]\n",
        "        if i < len(disp):\n",
        "            img = disp[i].permute(1, 2, 0).numpy()\n",
        "            true_lbl = classes[labels[i].item()]\n",
        "            pred_lbl = classes[preds[i].item()]\n",
        "            color = \"green\" if pred_lbl == true_lbl else \"red\"\n",
        "\n",
        "            ax.imshow(img)\n",
        "            ax.set_title(f\"Pred: {pred_lbl}\\nTrue: {true_lbl}\", color=color, fontsize=9)\n",
        "            ax.axis(\"off\")\n",
        "        else:\n",
        "            ax.axis(\"off\")\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "# ------------------------------\n",
        "# Visualization: Confusion Matrix (no sklearn)\n",
        "# ------------------------------\n",
        "def plot_confusion_matrix(model, loader, classes, device=\"cuda\", normalize=False, cmap=\"Blues\"):\n",
        "    \"\"\"\n",
        "    Computes and plots a confusion matrix across the entire loader.\n",
        "    Works without scikit-learn.\n",
        "    \"\"\"\n",
        "    device = device if (device == \"cuda\" and torch.cuda.is_available()) else \"cpu\"\n",
        "    model.to(device)\n",
        "    model.eval()\n",
        "\n",
        "    num_classes = len(classes)\n",
        "    cm = torch.zeros((num_classes, num_classes), dtype=torch.int64)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for x, y in loader:\n",
        "            x = x.to(device, non_blocking=True)\n",
        "            y = y.to(device, non_blocking=True)\n",
        "            preds = model(x).argmax(dim=1)\n",
        "            for t, p in zip(y.view(-1), preds.view(-1)):\n",
        "                cm[t.long(), p.long()] += 1\n",
        "\n",
        "    cm_np = cm.numpy() # Keep as int64 initially\n",
        "    if normalize:\n",
        "        cm_np = cm_np.astype(np.float64) # Only cast to float if normalizing\n",
        "        row_sums = cm_np.sum(axis=1, keepdims=True)\n",
        "        row_sums[row_sums == 0] = 1\n",
        "        cm_np = cm_np / row_sums\n",
        "\n",
        "    fig, ax = plt.subplots(figsize=(7, 6))\n",
        "    im = ax.imshow(cm_np, interpolation=\"nearest\", cmap=cmap)\n",
        "    plt.colorbar(im, ax=ax, fraction=0.046, pad=0.04)\n",
        "\n",
        "    ax.set(\n",
        "        xticks=np.arange(num_classes),\n",
        "        yticks=np.arange(num_classes),\n",
        "        xticklabels=classes,\n",
        "        yticklabels=classes,\n",
        "        ylabel=\"True label\",\n",
        "        xlabel=\"Predicted label\",\n",
        "        title=\"Confusion Matrix (normalized)\" if normalize else \"Confusion Matrix\",\n",
        "    )\n",
        "    plt.setp(ax.get_xticklabels(), rotation=45, ha=\"right\", rotation_mode=\"anchor\")\n",
        "\n",
        "    # annotate cells\n",
        "    fmt = \".2f\" if normalize else \"d\"\n",
        "    thresh = cm_np.max() / 2.0\n",
        "    for i in range(num_classes):\n",
        "        for j in range(num_classes):\n",
        "            ax.text(j, i, format(cm_np[i, j], fmt),\n",
        "                    ha=\"center\", va=\"center\",\n",
        "                    color=\"white\" if cm_np[i, j] > thresh else \"black\")\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "# ------------------------------\n",
        "# Define a simple model\n",
        "# ------------------------------\n",
        "class TinyCNN(nn.Module):\n",
        "    def __init__(self, num_classes=10):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Conv2d(3, 64, 3, padding=1), nn.ReLU(),\n",
        "            nn.Conv2d(64, 64, 3, padding=1), nn.ReLU(),\n",
        "            nn.MaxPool2d(2),                 # 16x16\n",
        "            nn.Conv2d(64, 128, 3, padding=1), nn.ReLU(),\n",
        "            nn.AdaptiveAvgPool2d((1,1)),\n",
        "        )\n",
        "        self.fc = nn.Linear(128, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.net(x)\n",
        "        x = torch.flatten(x, 1)\n",
        "        return self.fc(x)\n",
        "\n",
        "\n",
        "# ------------------------------\n",
        "# Orchestrate: Get Data -> Train -> Plot -> Visualize\n",
        "# ------------------------------\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(\"Device:\", device)\n",
        "\n",
        "# 1) Data\n",
        "worker_loaders, test_loader, classes, mean, std = get_data(\n",
        "    num_workers=4,     # number of train shards\n",
        "    batch_size=128\n",
        ")\n",
        "\n",
        "# Pick one shard for training (simple). For more data per epoch, loop over all shards.\n",
        "train_loader = worker_loaders[0]\n",
        "\n",
        "# 2) Model\n",
        "model = TinyCNN().to(device)\n",
        "\n",
        "# 3) Train & Collect History\n",
        "history, model = train_with_history(\n",
        "    model,\n",
        "    train_loader=train_loader,\n",
        "    val_loader=test_loader,   # for demo; for real use, make a proper validation split\n",
        "    epochs=5,\n",
        "    lr=3e-4,\n",
        "    weight_decay=1e-4,\n",
        "    device=device\n",
        ")\n",
        "\n",
        "# 4) Plot Training Curves\n",
        "plot_training_curves(history, title_prefix=\"CIFAR-10 (TinyCNN)\")\n",
        "\n",
        "# 5) Visualize Predictions vs Labels\n",
        "show_preds_vs_labels(\n",
        "    model,\n",
        "    loader=test_loader,\n",
        "    classes=classes,\n",
        "    mean=mean,\n",
        "    std=std,\n",
        "    device=device,\n",
        "    max_images=16,\n",
        "    nrow=4\n",
        ")\n",
        "\n",
        "# 6) Confusion Matrix\n",
        "plot_confusion_matrix(\n",
        "    model,\n",
        "    loader=test_loader,\n",
        "    classes=classes,\n",
        "    device=device,\n",
        "    normalize=False\n",
        ")\n"
      ],
      "metadata": {
        "id": "E7PO2Z8BdfAB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Analysis:\n",
        "**num_workers=4**\n",
        "  - Spawns 4 worker processes to prepare each batch in parallel (decode, augment, collate).\n",
        "\n",
        "**batch_size=128**\n",
        "  - This is the number of samples per batch that the DataLoader yields.\n",
        "  - In DDP (multiâ€‘GPU, one process per GPU), this is **per-process / per-GPU**.\n",
        "  "
      ],
      "metadata": {
        "id": "EE8hy5tHc2xh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "======================================================="
      ],
      "metadata": {
        "id": "Ln80JxUufPkj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Distributed ML system\n",
        "\n",
        "*   CPU DDP â€” Epoch Time vs Processes\n",
        "*   Projected Speedup (Ring All-Reduce Model)\n",
        "*   Projected Communication Fraction per Iteration\n",
        "*   Simulated Communication Slowdown on 1 GPU (Virtual GPUs)\n",
        "\n",
        "âœ…Key Insight\n",
        "  - Parallelism is only beneficial until communication costs overtake compute savings.\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "rl1LRLarfS6-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# ================================\n",
        "# Distributed Metrics & Plots Cell\n",
        "# ================================\n",
        "import os, json, time, sys, subprocess, textwrap, shutil, re\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# --------- Utilities: write helper scripts to disk ---------\n",
        "def write_file(path, content):\n",
        "    with open(path, \"w\", encoding=\"utf-8\") as f:\n",
        "        f.write(textwrap.dedent(content).lstrip())\n",
        "\n",
        "# --------- Script: gpu_baseline.py (single-GPU throughput) ---------\n",
        "gpu_baseline_py = r\"\"\"\n",
        "import time\n",
        "import torch, torch.nn as nn\n",
        "import json # Added import\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import datasets, transforms\n",
        "\n",
        "class TinyCNN(nn.Module):\n",
        "    def __init__(self, num_classes=10):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Conv2d(3, 64, 3, padding=1), nn.ReLU(),\n",
        "            nn.Conv2d(64, 64, 3, padding=1), nn.ReLU(),\n",
        "            nn.MaxPool2d(2),\n",
        "            nn.Conv2d(64, 128, 3, padding=1), nn.ReLU(),\n",
        "            nn.AdaptiveAvgPool2d((1,1)),\n",
        "        )\n",
        "        self.fc = nn.Linear(128, num_classes)\n",
        "    def forward(self, x):\n",
        "        x = self.net(x)\n",
        "        x = torch.flatten(x, 1)\n",
        "        return self.fc(x)\n",
        "\n",
        "def build_loader(batch_size=256, num_workers=2):\n",
        "    mean = (0.4914, 0.4822, 0.4465)\n",
        "    std  = (0.2470, 0.2435, 0.2616)\n",
        "    tf = transforms.Compose([\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(mean, std),\n",
        "    ])\n",
        "    ds = datasets.CIFAR10(\"./data\", train=True, download=True, transform=tf)\n",
        "    pin = torch.cuda.is_available()\n",
        "    return DataLoader(ds, batch_size=batch_size, shuffle=True,\n",
        "                      num_workers=num_workers, pin_memory=pin, persistent_workers=(num_workers>0))\n",
        "\n",
        "@torch.no_grad()\n",
        "def measure_forward_compute(model, device, loader, warmup=20, steps=100):\n",
        "    model.eval()\n",
        "    it = iter(loader)\n",
        "    # Warmup\n",
        "    for _ in range(min(warmup, len(loader))):\n",
        "        x, _ = next(it); x = x.to(device, non_blocking=True)\n",
        "        y = model(x)\n",
        "        _ = y.sum().item()\n",
        "    # Measure forward-only time\n",
        "    it = iter(loader)\n",
        "    if device.type == \"cuda\":\n",
        "        start = torch.cuda.Event(enable_timing=True)\n",
        "        end   = torch.cuda.Event(enable_timing=True)\n",
        "        torch.cuda.synchronize()\n",
        "        start.record()\n",
        "        count = 0\n",
        "        for _ in range(min(steps, len(loader))):\n",
        "            x, _ = next(it); x = x.to(device, non_blocking=True)\n",
        "            y = model(x); _ = y.sum()\n",
        "            count += x.size(0)\n",
        "        end.record(); torch.cuda.synchronize()\n",
        "        total_ms = start.elapsed_time(end)\n",
        "        ms_per_iter = total_ms / max(1, min(steps, len(loader)))\n",
        "        img_per_sec = count / (total_ms / 1000.0)\n",
        "        return ms_per_iter, ms_per_iter, img_per_sec\n",
        "    else:\n",
        "        t0 = time.perf_counter(); count = 0\n",
        "        for _ in range(min(steps, len(loader))):\n",
        "            x, _ = next(it); x = x.to(device)\n",
        "            y = model(x); _ = y.sum().item()\n",
        "            count += x.size(0)\n",
        "        t1 = time.perf_counter()\n",
        "        total_ms = (t1 - t0) * 1000.0\n",
        "        ms_per_iter = total_ms / max(1, min(steps, len(loader)))\n",
        "        img_per_sec = count / (total_ms / 1000.0)\n",
        "        return ms_per_iter, ms_per_iter, img_per_sec\n",
        "\n",
        "def grad_bytes(model):\n",
        "    return sum(p.numel()*p.element_size() for p in model.parameters() if p.requires_grad)\n",
        "\n",
        "def main():\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    model = TinyCNN().to(device)\n",
        "    loader = build_loader(batch_size=256, num_workers=2)\n",
        "    torch.backends.cudnn.benchmark = True\n",
        "\n",
        "    compute_ms_per_iter, total_ms_per_iter, img_per_sec = measure_forward_compute(model, device, loader)\n",
        "    out = {\n",
        "        \"device\": str(device),\n",
        "        \"compute_ms_per_iter\": float(compute_ms_per_iter),\n",
        "        \"total_ms_per_iter\": float(total_ms_per_iter),\n",
        "        \"img_per_sec\": float(img_per_sec),\n",
        "        \"grad_bytes\": float(grad_bytes(model)),\n",
        "        \"batch_size\": loader.batch_size,\n",
        "    }\n",
        "    print(json.dumps(out))\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n",
        "\"\"\"\n",
        "\n",
        "# --------- Script: ddp_cpu_bench.py (CPU DDP epoch time vs processes) ---------\n",
        "ddp_cpu_bench_py = r\"\"\"\n",
        "import os, time, json\n",
        "import torch, torch.nn as nn, torch.optim as optim\n",
        "import torch.distributed as dist\n",
        "import torch.multiprocessing as mp\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.utils.data.distributed import DistributedSampler\n",
        "from torchvision import datasets, transforms\n",
        "\n",
        "class TinyLinear(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(nn.Flatten(), nn.Linear(3*32*32, 10))\n",
        "    def forward(self, x): return self.net(x)\n",
        "\n",
        "def setup(rank, world_size):\n",
        "    os.environ.setdefault(\"MASTER_ADDR\",\"127.0.0.1\")\n",
        "    os.environ.setdefault(\"MASTER_PORT\",\"29555\")\n",
        "    dist.init_process_group(backend=\"gloo\", rank=rank, world_size=world_size)\n",
        "\n",
        "def cleanup():\n",
        "    dist.destroy_process_group()\n",
        "\n",
        "def run(rank, world_size, epochs=1, batch_size=512, num_workers=0):\n",
        "    setup(rank, world_size)\n",
        "    device = torch.device(\"cpu\")\n",
        "    tf = transforms.ToTensor()\n",
        "    train_ds = datasets.CIFAR10(\"./data\", train=True, download=True, transform=tf)\n",
        "    sampler  = DistributedSampler(train_ds, num_replicas=world_size, rank=rank, shuffle=True)\n",
        "    loader   = DataLoader(train_ds, batch_size=batch_size, sampler=sampler, num_workers=num_workers)\n",
        "\n",
        "    model = TinyLinear().to(device)\n",
        "    ddp   = nn.parallel.DistributedDataParallel(model)  # CPU DDP with Gloo\n",
        "    opt   = optim.SGD(ddp.parameters(), lr=0.1)\n",
        "    lossf = nn.CrossEntropyLoss()\n",
        "\n",
        "    # Warmup\n",
        "    sampler.set_epoch(0)\n",
        "    for x,y in loader:\n",
        "        x,y = x.to(device), y.to(device)\n",
        "        opt.zero_grad()\n",
        "        loss = lossf(ddp(x), y); loss.backward(); opt.step()\n",
        "        break\n",
        "\n",
        "    # Timed epoch\n",
        "    sampler.set_epoch(1)\n",
        "    t0 = time.perf_counter(); total=0\n",
        "    for x,y in loader:\n",
        "        x,y = x.to(device), y.to(device)\n",
        "        opt.zero_grad()\n",
        "        loss = lossf(ddp(x), y); loss.backward(); opt.step()\n",
        "        total += y.size(0)\n",
        "    t1 = time.perf_counter()\n",
        "\n",
        "    # Gather max time (epoch time = slowest rank)\n",
        "    epoch_s = torch.tensor([t1-t0], dtype=torch.float64)\n",
        "    dist.all_reduce(epoch_s, op=dist.ReduceOp.MAX)\n",
        "    if rank == 0:\n",
        "        print(json.dumps({\"world_size\": world_size, \"epoch_time_s\": float(epoch_s.item()), \"batch_size\": batch_size, \"samples\": total*world_size}))\n",
        "    cleanup()\n",
        "\n",
        "def launch(world_size):\n",
        "    mp.spawn(run, args=(world_size,), nprocs=world_size, join=True)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    import argparse\n",
        "    ap = argparse.ArgumentParser()\n",
        "    ap.add_argument(\"--world-size\", type=int, required=True)\n",
        "    args = ap.parse_args()\n",
        "    launch(args.world_size)\n",
        "\"\"\"\n",
        "\n",
        "# --------- Script: projection_utils.py (analytical model) ---------\n",
        "projection_utils_py = r\"\"\"\n",
        "import numpy as np\n",
        "\n",
        "def ring_allreduce_bytes_per_rank(total_grad_bytes: int, N: int) -> float:\n",
        "    # Each rank communicates about 2*(N-1)/N * total_grad_bytes per step\n",
        "    return total_grad_bytes * (2.0*(N-1)/N)\n",
        "\n",
        "def projected_iter_time_ms(N, compute_ms_1gpu, grad_bytes, bandwidth_GBps=25.0, latency_ms=0.05):\n",
        "    '''\n",
        "    Simple model: T(N) = T_compute/N + T_comm(N)\n",
        "     - T_compute from 1-GPU measured compute_ms_1gpu (forward-only or fw+bw â€” be consistent)\n",
        "     - T_comm(N) = bytes_per_rank(N)/BW + latency\n",
        "    '''\n",
        "    comm_bytes = ring_allreduce_bytes_per_rank(grad_bytes, N)\n",
        "    comm_ms = (comm_bytes / (bandwidth_GBps * 1e9)) * 1000.0 + latency_ms\n",
        "    return compute_ms_1gpu / N + comm_ms, comm_ms\n",
        "\n",
        "def project_speedup_efficiency(Ns, compute_ms_1gpu, grad_bytes, bandwidth_GBps=25.0, latency_ms=0.05):\n",
        "    T1 = compute_ms_1gpu  # assume no comm for N=1\n",
        "    TNs = []\n",
        "    comm_ms_list = []\n",
        "    for N in Ns:\n",
        "        TN, comm_ms = projected_iter_time_ms(N, compute_ms_1gpu, grad_bytes, bandwidth_GBps, latency_ms)\n",
        "        TNs.append(TN); comm_ms_list.append(comm_ms)\n",
        "    TNs = np.array(TNs); comm_ms_list = np.array(comm_ms_list)\n",
        "    S = T1 / TNs\n",
        "    E = S / Ns\n",
        "    comm_frac = comm_ms_list / TNs\n",
        "    return TNs, S, E, comm_ms_list, comm_frac\n",
        "\"\"\"\n",
        "\n",
        "# --------- Script: simulate_comm_on_1gpu.py (optional simulation) ---------\n",
        "simulate_comm_py = r\"\"\"\n",
        "import time, json, torch, torch.nn as nn\n",
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data import DataLoader\n",
        "from projection_utils import ring_allreduce_bytes_per_rank\n",
        "\n",
        "class TinyCNN(nn.Module):\n",
        "    def __init__(self, num_classes=10):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Conv2d(3,64,3,padding=1), nn.ReLU(),\n",
        "            nn.Conv2d(64,64,3,padding=1), nn.ReLU(),\n",
        "            nn.MaxPool2d(2),\n",
        "            nn.Conv2d(64,128,3,padding=1), nn.ReLU(),\n",
        "            nn.AdaptiveAvgPool2d((1,1)),\n",
        "        )\n",
        "        self.fc = nn.Linear(128, num_classes)\n",
        "    def forward(self, x):\n",
        "        x = self.net(x)\n",
        "        x = torch.flatten(x,1)\n",
        "        return self.fc(x)\n",
        "\n",
        "def grad_bytes(model):\n",
        "    return sum(p.numel()*p.element_size() for p in model.parameters() if p.requires_grad)\n",
        "\n",
        "def build_loader(batch=256, workers=2):\n",
        "    mean=(0.4914,0.4822,0.4465); std=(0.2470,0.2435,0.2616)\n",
        "    tf = transforms.Compose([transforms.ToTensor(), transforms.Normalize(mean,std)])\n",
        "    ds = datasets.CIFAR10(\"./data\", train=True, download=True, transform=tf)\n",
        "    return DataLoader(ds, batch_size=batch, shuffle=True, num_workers=workers, pin_memory=torch.cuda.is_available(), persistent_workers=(workers>0))\n",
        "\n",
        "def simulate_comm_delay_s(grad_bytes, N, bandwidth_GBps=25.0, latency_ms=0.05):\n",
        "    comm_bytes = ring_allreduce_bytes_per_rank(grad_bytes, N)\n",
        "    return (comm_bytes / (bandwidth_GBps*1e9)) + (latency_ms/1000.0)\n",
        "\n",
        "@torch.no_grad()\n",
        "def run_virtual_scaling(device=\"cuda\", steps=120, Ns=(1,2,4,8), bandwidth_GBps=25.0):\n",
        "    device = torch.device(device if (device==\"cuda\" and torch.cuda.is_available()) else \"cpu\")\n",
        "    model = TinyCNN().to(device).eval()\n",
        "    loader = build_loader(batch=256, workers=2)\n",
        "    gbytes = grad_bytes(model)\n",
        "\n",
        "    it = iter(loader)\n",
        "    results = []\n",
        "    # Warmup few batches\n",
        "    for _ in range(10):\n",
        "        x,_ = next(it); x = x.to(device, non_blocking=True)\n",
        "        _ = model(x).sum()\n",
        "\n",
        "    for N in Ns:\n",
        "        it = iter(loader)\n",
        "        t0 = time.perf_counter(); count=0\n",
        "        for _ in range(steps):\n",
        "            x,_ = next(it); x = x.to(device, non_blocking=True)\n",
        "            _ = model(x).sum()\n",
        "            time.sleep(simulate_comm_delay_s(gbytes, N, bandwidth_GBps=bandwidth_GBps))\n",
        "            count += x.size(0)\n",
        "        t1 = time.perf_counter()\n",
        "        ips = count/(t1-t0)\n",
        "        results.append({\"N\":int(N), \"img_per_sec\": float(ips)})\n",
        "    print(json.dumps(results))\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    dev = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "    run_virtual_scaling(device=dev)\n",
        "\"\"\"\n",
        "\n",
        "# --------- Write the files ---------\n",
        "write_file(\"gpu_baseline.py\", gpu_baseline_py)\n",
        "write_file(\"ddp_cpu_bench.py\", ddp_cpu_bench_py)\n",
        "write_file(\"projection_utils.py\", projection_utils_py)\n",
        "write_file(\"simulate_comm_on_1gpu.py\", simulate_comm_py)\n",
        "\n",
        "# --------- Run GPU baseline ---------\n",
        "print(\">>> Running GPU baseline ...\")\n",
        "p = subprocess.run([sys.executable, \"gpu_baseline.py\"], capture_output=True, text=True)\n",
        "if p.returncode != 0:\n",
        "    print(p.stderr)\n",
        "    raise RuntimeError(\"gpu_baseline.py failed\")\n",
        "gpu_line = p.stdout.strip().splitlines()[-1]\n",
        "gpu_base = json.loads(gpu_line)\n",
        "compute_ms_1gpu = gpu_base[\"compute_ms_per_iter\"]\n",
        "grad_bytes = gpu_base[\"grad_bytes\"]\n",
        "img_per_sec_1gpu = gpu_base[\"img_per_sec\"]\n",
        "print(\"GPU baseline:\", gpu_base)\n",
        "\n",
        "# Plot A: GPU baseline throughput\n",
        "plt.figure(figsize=(5,4))\n",
        "plt.bar([\"1Ã—GPU\"], [img_per_sec_1gpu], color=\"#4e79a7\")\n",
        "plt.ylabel(\"Images / second\")\n",
        "plt.title(\"GPU Baseline Throughput\")\n",
        "plt.grid(axis=\"y\", alpha=0.3)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# --------- Run CPU DDP epoch time vs processes ---------\n",
        "print(\">>> Running CPU DDP bench (1, 2, 4 processes; 4 may be skipped if too heavy) ...\")\n",
        "cpu_records = []\n",
        "for ws in [1, 2, 4]:\n",
        "    print(f\"  - world_size={ws}\")\n",
        "    try:\n",
        "        p = subprocess.run([sys.executable, \"ddp_cpu_bench.py\", \"--world-size\", str(ws)],\n",
        "                           capture_output=True, text=True, timeout=900)\n",
        "        if p.returncode != 0:\n",
        "            print(f\"    returned non-zero (skipping):\", p.stderr.splitlines()[-1] if p.stderr else \"no stderr\")\n",
        "            continue\n",
        "        # collect all json lines\n",
        "        for line in p.stdout.splitlines():\n",
        "            line = line.strip()\n",
        "            if not line: continue\n",
        "            try:\n",
        "                rec = json.loads(line)\n",
        "                cpu_records.append(rec)\n",
        "            except json.JSONDecodeError:\n",
        "                pass\n",
        "    except Exception as e:\n",
        "        print(f\"    exception (skipping {ws}): {e}\")\n",
        "\n",
        "cpu_records = sorted(cpu_records, key=lambda r: r[\"world_size\"])\n",
        "print(\"CPU DDP records:\", cpu_records)\n",
        "\n",
        "if cpu_records:\n",
        "    # Plot B: CPU DDP epoch time vs processes\n",
        "    plt.figure(figsize=(6,4))\n",
        "    plt.plot([r[\"world_size\"] for r in cpu_records],\n",
        "             [r[\"epoch_time_s\"] for r in cpu_records],\n",
        "             marker=\"o\", label=\"Measured (CPU DDP)\")\n",
        "    plt.xlabel(\"Processes (world size)\")\n",
        "    plt.ylabel(\"Epoch time (s)\")\n",
        "    plt.title(\"CPU DDP â€” Epoch Time vs Processes\")\n",
        "    plt.grid(True, alpha=0.3); plt.legend()\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "else:\n",
        "    print(\"No CPU DDP records gathered; skipping Plot B.\")\n",
        "\n",
        "# --------- Projected speedup & communication fraction (analytical) ---------\n",
        "from projection_utils import project_speedup_efficiency\n",
        "Ns = np.array([1,2,4,8])\n",
        "bandwidth_GBps = 25.0   # adjust if you want to model PCIe/NVLink differences\n",
        "latency_ms = 0.05\n",
        "\n",
        "TNs, S, E, comm_ms_list, comm_frac = project_speedup_efficiency(\n",
        "    Ns, compute_ms_1gpu=compute_ms_1gpu, grad_bytes=grad_bytes,\n",
        "    bandwidth_GBps=bandwidth_GBps, latency_ms=latency_ms\n",
        ")\n",
        "\n",
        "# Plot C: Projected speedup vs GPUs  |  Plot D: Communication fraction vs GPUs\n",
        "plt.figure(figsize=(10,4))\n",
        "plt.subplot(1,2,1)\n",
        "plt.plot(Ns, S, marker=\"o\", label=\"Projected Speedup\")\n",
        "plt.plot(Ns, Ns, \"k--\", alpha=0.6, label=\"Ideal\")\n",
        "plt.xlabel(\"GPUs (N)\"); plt.ylabel(\"Speedup S(N)\")\n",
        "plt.title(\"Projected Speedup (ring all-reduce model)\")\n",
        "plt.grid(True, alpha=0.3); plt.legend()\n",
        "\n",
        "plt.subplot(1,2,2)\n",
        "plt.plot(Ns, comm_frac*100, marker=\"o\", color=\"#e15759\", label=\"Comm fraction (%)\")\n",
        "plt.xlabel(\"GPUs (N)\"); plt.ylabel(\"Comm fraction (%)\")\n",
        "plt.title(\"Projected Communication Fraction per Iteration\")\n",
        "plt.grid(True, alpha=0.3); plt.legend()\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# --------- (Optional) Simulated communication slowdown on 1 GPU ---------\n",
        "print(\">>> Running simulated communication slowdown on 1 GPU/CPU ...\")\n",
        "try:\n",
        "    p = subprocess.run([sys.executable, \"simulate_comm_on_1gpu.py\"],\n",
        "                       capture_output=True, text=True, timeout=900)\n",
        "    if p.returncode == 0:\n",
        "        sim_res = json.loads(p.stdout.strip().splitlines()[-1])\n",
        "        Ns_sim = [r[\"N\"] for r in sim_res]\n",
        "        IPS_sim = [r[\"img_per_sec\"] for r in sim_res]\n",
        "        plt.figure(figsize=(6,4))\n",
        "        plt.plot(Ns_sim, IPS_sim, marker=\"o\", label=\"Simulated throughput\")\n",
        "        plt.xlabel(\"Virtual GPUs (N)\")\n",
        "        plt.ylabel(\"Images / second\")\n",
        "        plt.title(\"Simulated Communication Slowdown on 1 GPU\")\n",
        "        plt.grid(True, alpha=0.3); plt.legend()\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "    else:\n",
        "        print(\"Simulation script failed:\", p.stderr.splitlines()[-1] if p.stderr else \"no stderr\")\n",
        "except Exception as e:\n",
        "    print(\"Simulation step skipped due to error:\", e)\n",
        "\n",
        "print(\"\\nDone. You now have all five plots:\")\n",
        "print(\"  1) GPU baseline throughput\")\n",
        "print(\"  2) CPU DDP epoch time vs processes\")\n",
        "print(\"  3) Projected speedup vs GPUs\")\n",
        "print(\"  4) Communication fraction vs GPUs\")\n",
        "print(\"  5) Simulated communication slowdown (optional)\")\n"
      ],
      "metadata": {
        "id": "kctNsOGMCKeT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "âš¡**Analysis**\n",
        "\n",
        "- CPU DDP â€” Epoch Time vs Processes\n",
        "  **Observation**\n",
        "  - 1 process: ~9.8 s / epoch\n",
        "  - 2 processes: ~7.9 s / epoch (best)\n",
        "  - 4 processes: ~9.3 s / epoch (worse than 2, close to 1)\n",
        "  - Moving from 2 â†’ 4 processes increases epoch time, indicating:\n",
        "    - Communication overhead dominates\n",
        "      - Possible causes:\n",
        "        - Gradient synchronization cost\n",
        "        - CPU memory bandwidth saturation\n",
        "        - Context switching and NUMA effects\n",
        "        - Inefficient inter-process communication\n",
        "\n",
        "ðŸ‘  Communication fraction increases almost linearly:\n",
        "  - 1 GPU: ~0.5%\n",
        "  - 2 GPUs: ~1.2%\n",
        "  - 4 GPUs: ~2.7%\n",
        "  - 8 GPUs: ~5.8%\n",
        "- Even with ring all-reduce, synchronization cost grows with N\n",
        "- Communication becomes a scaling limiter beyond a certain GPU count\n",
        "\n",
        "â™¦**Inference of Graphs:**\n",
        "\n",
        "\n",
        "1.   CPU-based DDP shows sub-linear scaling and has a sweet spot at 2 processes. Beyond that, overhead outweighs parallelism benefits.\n",
        "2.   While speedup may look linear, efficiency drops as communication becomes a larger fraction of iteration time\n",
        "1.   Even on a single physical GPU, excessive parallelism hurts throughput if communication/synchronization is introduced.\n",
        "2.   The model represents an ideal upper bound\n",
        "\n",
        "\n",
        "âœ… Final Takeaway\n",
        "\n",
        "  - CPU parallelism is limited and quickly communication-bound\n",
        "  - GPU parallelism scales well in theory but is constrained by synchronization costs\n",
        "  - More workers â‰  faster training\n",
        "  - Optimal performance comes from balanced compute, memory, and communication"
      ],
      "metadata": {
        "id": "ZcVllkfNfnua"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "========================================================="
      ],
      "metadata": {
        "id": "KGUVqhw1kXLY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Analysis of Throughput and Peak Memory (Process RSS)\n",
        "\n",
        "\n",
        "\n",
        "*   **Throughput** measures how many training samples (images) are processed per second during training. It is a direct indicator of training speed and hardware utilization.\n",
        "*   **Peak memory** represents the maximum RAM usage (Resident Set Size) during training. It reflects the memory footprint of the training process, including model parameters, activations, buffers, and framework overhead.\n",
        "\n"
      ],
      "metadata": {
        "id": "wvA6D2GDkY2x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Optional dependency â€” used if available\n",
        "try:\n",
        "    import psutil\n",
        "except Exception:\n",
        "    psutil = None\n",
        "\n",
        "import matplotlib\n",
        "matplotlib.rcParams[\"figure.dpi\"] = 120\n",
        "\n",
        "\n",
        "def suggested_num_workers(memory_guard=True):\n",
        "    \"\"\"\n",
        "    Suggest a DataLoader num_workers based on CPU cores and (optionally) memory.\n",
        "    Conservative heuristic suitable for CIFAR-10 (small samples).\n",
        "    \"\"\"\n",
        "    cores = os.cpu_count() or 2\n",
        "    workers = min(8, max(2, cores // 2))\n",
        "    if memory_guard and psutil:\n",
        "        try:\n",
        "            vm = psutil.virtual_memory()\n",
        "            # Keep some headroom if RAM < 8GB\n",
        "            if vm.total < 8 * 1024**3:\n",
        "                workers = min(workers, 4)\n",
        "        except Exception:\n",
        "            pass\n",
        "    return workers\n",
        "\n",
        "# ----------------------------\n",
        "# 2) Model\n",
        "# ----------------------------\n",
        "class ResNet50CIFAR(nn.Module):\n",
        "    def __init__(self) -> None:\n",
        "        super().__init__()\n",
        "        base = models.resnet50(weights=None)\n",
        "        in_features = base.fc.in_features\n",
        "        base.fc = nn.Linear(in_features, 10)\n",
        "        self.model = base\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        return self.model(x)\n",
        "\n",
        "# ----------------------------\n",
        "# 3) Automatic Batch-Size Finder\n",
        "# ----------------------------\n",
        "@torch.no_grad()\n",
        "def _try_one_step_for_batch(model, device, batch_size, num_classes=10,\n",
        "                            input_size=(3, 32, 32), amp=False) -> bool:\n",
        "    \"\"\"\n",
        "    Try to allocate and run a single forward+backward with the given batch size.\n",
        "    Returns True if successful, False if OOM (or runtime error).\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # Create a dummy batch\n",
        "        x = torch.randn((batch_size, *input_size), device=device)\n",
        "        y = torch.randint(0, num_classes, (batch_size,), device=device)\n",
        "        model.train()\n",
        "        # Enable grad for the test step (even though we decorated with no_grad above)\n",
        "        # We need gradients to allocate graph size close to training reality.\n",
        "        torch.set_grad_enabled(True)\n",
        "        criterion = nn.CrossEntropyLoss().to(device)\n",
        "        # Simple optimizer to create param states\n",
        "        opt = optim.SGD(model.parameters(), lr=1e-3, momentum=0.9)\n",
        "        use_amp = amp and device.type == \"cuda\"\n",
        "        scaler = torch.amp.GradScaler(\"cuda\", enabled=use_amp)\n",
        "\n",
        "        opt.zero_grad(set_to_none=True)\n",
        "        if device.type == \"cuda\":\n",
        "            torch.cuda.reset_peak_memory_stats()\n",
        "        use_amp = amp and device.type == \"cuda\"\n",
        "        with torch.amp.autocast(\"cuda\", enabled=use_amp):\n",
        "            out = model(x)\n",
        "            loss = criterion(out, y)\n",
        "        scaler.scale(loss).backward() if scaler.is_enabled() else loss.backward()\n",
        "        scaler.step(opt) if scaler.is_enabled() else opt.step()\n",
        "        if scaler.is_enabled():\n",
        "            scaler.update()\n",
        "\n",
        "        # Clean up\n",
        "        del x, y, out, loss, opt, criterion, scaler\n",
        "        torch.set_grad_enabled(False)\n",
        "        if device.type == \"cuda\":\n",
        "            torch.cuda.empty_cache()\n",
        "        return True\n",
        "    except RuntimeError as e:\n",
        "        msg = str(e).lower()\n",
        "        # CUDA OOM or MPS OOM often contains \"out of memory\"\n",
        "        if (\"out of memory\" in msg) or (\"cublas\" in msg and \"alloc\" in msg):\n",
        "            if device.type == \"cuda\":\n",
        "                torch.cuda.empty_cache()\n",
        "            torch.set_grad_enabled(False)\n",
        "            return False\n",
        "        # Other runtime errorsâ€”treat as failure for safety\n",
        "        torch.set_grad_enabled(False)\n",
        "        if device.type == \"cuda\":\n",
        "            torch.cuda.empty_cache()\n",
        "        return False\n",
        "    except Exception:\n",
        "        torch.set_grad_enabled(False)\n",
        "        if device.type == \"cuda\":\n",
        "            torch.cuda.empty_cache()\n",
        "        return False\n",
        "\n",
        "def find_max_batch_size(model: nn.Module,\n",
        "                        device: torch.device,\n",
        "                        start_bs: int = 64,\n",
        "                        max_limit: int = 4096,\n",
        "                        amp: bool = True,\n",
        "                        input_size: Tuple[int, int, int] = (3, 32, 32)) -> int:\n",
        "    \"\"\"\n",
        "    Exponential search + binary search to find the largest batch size\n",
        "    that can run a single training step without OOM.\n",
        "    For CPU, we keep a conservative cap to avoid thrashing.\n",
        "    \"\"\"\n",
        "    if device.type == \"cpu\":\n",
        "        # Keep it conservative for CPU to avoid excessive paging\n",
        "        return min(256, start_bs)\n",
        "\n",
        "    # Warm-up on small batch\n",
        "    ok = _try_one_step_for_batch(model, device, start_bs, input_size=input_size, amp=amp)\n",
        "    if not ok:\n",
        "        # Reduce further if even start fails\n",
        "        bs = max(8, start_bs // 2)\n",
        "        while bs >= 8 and not _try_one_step_for_batch(model, device, bs, input_size=input_size, amp=amp):\n",
        "            bs //= 2\n",
        "        return bs\n",
        "\n",
        "    # Exponential growth\n",
        "    low = start_bs\n",
        "    high = start_bs\n",
        "    while high < max_limit:\n",
        "        candidate = high * 2\n",
        "        ok = _try_one_step_for_batch(model, device, candidate, input_size=input_size, amp=amp)\n",
        "        if ok:\n",
        "            high = candidate\n",
        "        else:\n",
        "            break\n",
        "\n",
        "    if high >= max_limit:\n",
        "        return high\n",
        "\n",
        "    # Binary search between (low=high) and (bad=next double)\n",
        "    bad = high * 2\n",
        "    while low < high:\n",
        "        mid = (low + high + 1) // 2\n",
        "        ok = _try_one_step_for_batch(model, device, mid, input_size=input_size, amp=amp)\n",
        "        if ok:\n",
        "            low = mid\n",
        "        else:\n",
        "            high = mid - 1\n",
        "\n",
        "    return low\n",
        "\n",
        "# ----------------------------\n",
        "# 4) Training Utilities\n",
        "# ----------------------------\n",
        "def set_seed(seed: int = 123):\n",
        "    random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "\n",
        "def make_device():\n",
        "    if torch.cuda.is_available():\n",
        "        return torch.device(\"cuda\")\n",
        "    if hasattr(torch.backends, \"mps\") and torch.backends.mps.is_available():\n",
        "        return torch.device(\"mps\")\n",
        "    return torch.device(\"cpu\")\n",
        "\n",
        "def make_dataloaders(batch_size: int,\n",
        "                     device: torch.device,\n",
        "                     workers: Optional[int] = None):\n",
        "    tfm = transforms.Compose([\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(mean=(0.4914, 0.4822, 0.4465),\n",
        "                             std=(0.2470, 0.2435, 0.2616)),\n",
        "    ])\n",
        "    train_ds = datasets.CIFAR10(root=\"./data\", train=True, download=True, transform=tfm)\n",
        "    test_ds  = datasets.CIFAR10(root=\"./data\", train=False, download=True, transform=tfm)\n",
        "\n",
        "    if workers is None:\n",
        "        workers = suggested_num_workers()\n",
        "\n",
        "    pin_memory = (device.type == \"cuda\")\n",
        "    persistent_workers = (workers > 0)\n",
        "\n",
        "    train_dl = DataLoader(train_ds, batch_size=batch_size, shuffle=True,\n",
        "                          num_workers=workers, pin_memory=pin_memory,\n",
        "                          persistent_workers=persistent_workers, drop_last=True)\n",
        "    test_dl  = DataLoader(test_ds, batch_size=batch_size, shuffle=False,\n",
        "                          num_workers=workers, pin_memory=pin_memory,\n",
        "                          persistent_workers=persistent_workers, drop_last=False)\n",
        "    return train_dl, test_dl\n",
        "\n",
        "def train_for_images(model: nn.Module,\n",
        "                     train_dl: DataLoader,\n",
        "                     device: torch.device,\n",
        "                     target_images: int = 20480,\n",
        "                     lr: float = 0.1,\n",
        "                     amp: bool = True) -> Dict[str, Any]:\n",
        "    \"\"\"\n",
        "    Train just enough to process `target_images` images.\n",
        "    Collects:\n",
        "      - loss curve (per step)\n",
        "      - total time, throughput (images/sec)\n",
        "      - peak memory (VRAM for CUDA; RSS for CPU/MPS)\n",
        "    \"\"\"\n",
        "    model = model.to(device)\n",
        "    model.train()\n",
        "\n",
        "    criterion = nn.CrossEntropyLoss().to(device)\n",
        "    optimizer = optim.SGD(model.parameters(), lr=lr, momentum=0.9, weight_decay=5e-4)\n",
        "    use_amp = amp and device.type == \"cuda\"\n",
        "    scaler = torch.amp.GradScaler(\"cuda\", enabled = use_amp)\n",
        "\n",
        "    # Memory tracking\n",
        "    if device.type == \"cuda\":\n",
        "        torch.cuda.reset_peak_memory_stats()\n",
        "    proc = psutil.Process(os.getpid()) if psutil else None\n",
        "    peak_rss = proc.memory_info().rss if proc else 0\n",
        "\n",
        "    total_images = 0\n",
        "    losses = []\n",
        "    start = time.time()\n",
        "\n",
        "    # Training loop for a fixed number of images\n",
        "    while total_images < target_images:\n",
        "        for images, labels in train_dl:\n",
        "            images = images.to(device, non_blocking=True)\n",
        "            labels = labels.to(device, non_blocking=True)\n",
        "\n",
        "            optimizer.zero_grad(set_to_none=True)\n",
        "            use_amp = amp and device.type == \"cuda\"\n",
        "            with torch.amp.autocast(\"cuda\", enabled= use_amp):\n",
        "                outputs = model(images)\n",
        "                loss = criterion(outputs, labels)\n",
        "\n",
        "            if scaler.is_enabled():\n",
        "                scaler.scale(loss).backward()\n",
        "                scaler.step(optimizer)\n",
        "                scaler.update()\n",
        "            else:\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "\n",
        "            losses.append(loss.item())\n",
        "            processed = images.size(0)\n",
        "            total_images += processed\n",
        "\n",
        "            # Track peak RSS (CPU/MPS) if psutil is present\n",
        "            if proc:\n",
        "                rss = proc.memory_info().rss\n",
        "                if rss > peak_rss:\n",
        "                    peak_rss = rss\n",
        "\n",
        "            if total_images >= target_images:\n",
        "                break\n",
        "\n",
        "    end = time.time()\n",
        "    duration = end - start\n",
        "    images_sec = total_images / max(1e-6, duration)\n",
        "\n",
        "    # Peak memory\n",
        "    if device.type == \"cuda\":\n",
        "        peak_mem_bytes = torch.cuda.max_memory_allocated()\n",
        "        peak_mem_human = _bytes2human(peak_mem_bytes)\n",
        "    else:\n",
        "        peak_mem_bytes = peak_rss\n",
        "        peak_mem_human = _bytes2human(peak_mem_bytes)\n",
        "\n",
        "    return {\n",
        "        \"losses\": losses,\n",
        "        \"duration\": duration,\n",
        "        \"images\": total_images,\n",
        "        \"images_per_sec\": images_sec,\n",
        "        \"peak_mem_bytes\": peak_mem_bytes,\n",
        "        \"peak_mem_human\": peak_mem_human,\n",
        "    }\n",
        "\n",
        "# ----------------------------\n",
        "# 5) Main Experiment\n",
        "# ----------------------------\n",
        "def main():\n",
        "    set_seed(42)\n",
        "    # The print_system_summary function is not defined in this notebook.\n",
        "    # print_system_summary()\n",
        "\n",
        "    device = make_device()\n",
        "    print(\"Using device:\", device)\n",
        "\n",
        "    # Toggle AMP (only effective on CUDA)\n",
        "    use_amp = True\n",
        "\n",
        "    # ---- Baseline (Static batch size) ----\n",
        "    static_bs = 128\n",
        "    print(f\"\\n[Baseline] Using static batch size = {static_bs}\")\n",
        "    model_static = ResNet50CIFAR()\n",
        "\n",
        "    # Dataloaders for static\n",
        "    train_dl_static, _ = make_dataloaders(static_bs, device)\n",
        "\n",
        "    # Short training to a target number of images\n",
        "    target_images = 20480  # ~20k images for quick comparison; adjust as desired\n",
        "    stats_static = train_for_images(model_static, train_dl_static, device,\n",
        "                                    target_images=target_images, lr=0.1, amp=use_amp)\n",
        "\n",
        "    print(f\"[Baseline] images/sec: {stats_static['images_per_sec']:.1f}, \"\n",
        "          f\"time: {stats_static['duration']:.2f}s, \"\n",
        "          f\"peak mem: {stats_static['peak_mem_human']}\")\n",
        "\n",
        "    # ---- Auto batch size ----\n",
        "    print(\"\\n[Auto] Finding maximum batch size that fits...\")\n",
        "    model_auto = ResNet50CIFAR().to(device)\n",
        "    # Important: run finder on a FRESH model to reflect real graph size\n",
        "    # Also keep AMP setting consistent across runs\n",
        "    auto_bs = find_max_batch_size(model_auto, device, start_bs=static_bs, amp=use_amp)\n",
        "    if auto_bs < 8:\n",
        "        print(f\"[Auto] Could not find a larger batch size safely; using {auto_bs}.\")\n",
        "    else:\n",
        "        print(f\"[Auto] Selected batch size = {auto_bs}\")\n",
        "\n",
        "    # Re-create dataloader with auto batch size\n",
        "    train_dl_auto, _ = make_dataloaders(auto_bs, device)\n",
        "\n",
        "    # Re-init model (fresh weights) for fair comparison\n",
        "    model_auto = ResNet50CIFAR()\n",
        "    stats_auto = train_for_images(model_auto, train_dl_auto, device,\n",
        "                                  target_images=target_images, lr=0.1, amp=use_amp)\n",
        "\n",
        "    print(f\"[Auto] images/sec: {stats_auto['images_per_sec']:.1f}, \"\n",
        "          f\"time: {stats_auto['duration']:.2f}s, \"\n",
        "          f\"peak mem: {stats_auto['peak_mem_human']}\")\n",
        "\n",
        "    # ----------------------------\n",
        "    # 6) Plot Comparison\n",
        "    # ----------------------------\n",
        "    fig, axes = plt.subplots(1, 3, figsize=(16, 4.5))\n",
        "    fig.suptitle(\"Batch Size Strategy Comparison (ResNet-50 on CIFAR-10)\")\n",
        "\n",
        "    # A) Throughput bar\n",
        "    ax = axes[0]\n",
        "    methods = [\"Static\", \"Auto\"]\n",
        "    throughput = [stats_static[\"images_per_sec\"], stats_auto[\"images_per_sec\"]]\n",
        "    colors = [\"#4C78A8\", \"#F58518\"]\n",
        "    ax.bar(methods, throughput, color=colors)\n",
        "    ax.set_title(\"Throughput (images/sec)\")\n",
        "    ax.set_ylabel(\"images/sec\")\n",
        "    for i, v in enumerate(throughput):\n",
        "        ax.text(i, v, f\"{v:.0f}\", ha=\"center\", va=\"bottom\", fontsize=9)\n",
        "\n",
        "    # B) Loss vs Images Seen (k)\n",
        "    ax = axes[1]\n",
        "    # x-axis as cumulative images seen (in thousands)\n",
        "    x_static = [((i+1) * train_dl_static.batch_size) / 1000.0 for i in range(len(stats_static[\"losses\"]))]\n",
        "    x_auto   = [((i+1) * train_dl_auto.batch_size) / 1000.0 for i in range(len(stats_auto[\"losses\"]))]\n",
        "\n",
        "    ax.plot(x_static, stats_static[\"losses\"], label=f\"Static (bs={train_dl_static.batch_size})\", color=colors[0], alpha=0.9)\n",
        "    ax.plot(x_auto,   stats_auto[\"losses\"],   label=f\"Auto (bs={train_dl_auto.batch_size})\",   color=colors[1], alpha=0.9)\n",
        "    ax.set_title(\"Training Loss vs Images Seen (thousands)\")\n",
        "    ax.set_xlabel(\"Images seen (Ã—1k)\")\n",
        "    ax.set_ylabel(\"Loss\")\n",
        "    ax.grid(alpha=0.25)\n",
        "    ax.legend()\n",
        "\n",
        "    # C) Peak Memory\n",
        "    ax = axes[2]\n",
        "    mem_labels = [\"Static\", \"Auto\"]\n",
        "    mem_bytes = [stats_static[\"peak_mem_bytes\"], stats_auto[\"peak_mem_bytes\"]]\n",
        "    ax.bar(mem_labels, [b / (1024**3) for b in mem_bytes], color=colors)\n",
        "    ax.set_title(\"Peak Memory\")\n",
        "    if device.type == \"cuda\":\n",
        "        ax.set_ylabel(\"VRAM (GB)\")\n",
        "    else:\n",
        "        ax.set_ylabel(\"Process RSS (GB)\")\n",
        "    for i, b in enumerate(mem_bytes):\n",
        "        ax.text(i, b / (1024**3), _bytes2human(b), ha=\"center\", va=\"bottom\", fontsize=9)\n",
        "\n",
        "    plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "id": "xbIJXwOcdT-W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**1) Throughput (images/sec)**\n",
        "\n",
        "**Observation:** Both bars are â‰ˆ 31 img/s (difference is visually negligible).\n",
        "\n",
        "**Inference:**\n",
        "  - Auto likely landed on the same batch size (128), or it explored but found no higher effective batch that fits memory/latency targets.\n",
        "  - If you're on CPU or a bandwidth-limited GPU / small dataset, the bottleneck may be data loading or memory bandwidth, not batch size. Hence, higher batch sizes wouldn't increase images/sec.\n",
        "  - If Auto ended at 128, it's behaving as **expected given constraints.**\n",
        "\n",
        "**2) Training Loss vs Images Seen**\n",
        "\n",
        "**Observation:**\n",
        "  - Curves (Static vs Auto) are nearly overlapping with similar early spikes; both decay towards the same low loss.\n",
        "**Inference:**\n",
        "  - No change in optimization dynamics: same batch size + same schedule â†’ same trajectory.\n",
        "  - Spikes early on are normal for CIFAR-10 with ResNet-50 (data augmentation variance and learning-rate warmup/plateaus).\n",
        "  - remember to adjust LR via the linear-scaling rule and re-tune warmup; otherwise comparisons will be confounded.\n",
        "\n",
        "**3) Peak Memory (Process RSS)**\n",
        "\n",
        "**Observation:** Static â‰ˆ 3.01â€¯GB, Auto â‰ˆ 3.33â€¯GB â†’ approximate 0.32â€¯GB extra (~10â€“11%).\n",
        "\n",
        "**Inference:**\n",
        "  - Extra RSS is typical for auto batch sizing because it may:\n",
        "  - Run probe steps to detect the max safe batch.\n",
        "  - Keep additional dataloader/colllate buffers or caching states.\n",
        "  - Maintain bookkeeping for retries / gradient accumulation fallback.\n",
        "  - **Note:** Auto batch sizing incurs additional memory overhead without delivering performance benefits in this workload.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "2TAgJnqVk95E"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "========================================================="
      ],
      "metadata": {
        "id": "fqA2SAXpmBVr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## batch_size vs num_workers interact\n",
        "\n",
        "*   How the system (CPU, memory, I/O, GPU) impacts performance\n",
        "*   Training throughput is determined by two pipelines running in parallel:\n",
        "      - Data pipeline (CPU side) â†’ controlled mainly by num_workers\n",
        "      - Compute pipeline (GPU/CPU side) â†’ controlled mainly by batch_size\n",
        "- Performance improves only when both pipelines are balanced.\n",
        "\n"
      ],
      "metadata": {
        "id": "FYdcT3AmmDP9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Optional dependency â€” richer CPU memory metrics (not required)\n",
        "try:\n",
        "    import psutil\n",
        "except Exception:\n",
        "    psutil = None\n",
        "\n",
        "# TensorBoard logging\n",
        "try:\n",
        "    from torch.utils.tensorboard import SummaryWriter\n",
        "except Exception:\n",
        "    SummaryWriter = None  # We'll guard usage below.\n",
        "\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "matplotlib.rcParams[\"figure.dpi\"] = 120\n",
        "\n",
        "from torch.profiler import(\n",
        "    profile,\n",
        "    ProfilerActivity,\n",
        "    schedule,\n",
        "    tensorboard_trace_handler,\n",
        "    record_function\n",
        ")\n",
        "\n",
        "\n",
        "# ----------------------------\n",
        "# Model\n",
        "# ----------------------------\n",
        "class ResNet50CIFAR(nn.Module):\n",
        "    def __init__(self) -> None:\n",
        "        super().__init__()\n",
        "        base = models.resnet50(weights=None)\n",
        "        in_features = base.fc.in_features\n",
        "        base.fc = nn.Linear(in_features, 10)\n",
        "        self.model = base\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        return self.model(x)\n",
        "\n",
        "\n",
        "# ----------------------------\n",
        "# Utils\n",
        "# ----------------------------\n",
        "def set_seed(seed: int = 42):\n",
        "    random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "\n",
        "\n",
        "def device_auto() -> torch.device:\n",
        "    if torch.cuda.is_available():\n",
        "        return torch.device(\"cuda\")\n",
        "    if hasattr(torch.backends, \"mps\") and torch.backends.mps.is_available():\n",
        "        return torch.device(\"mps\")\n",
        "    return torch.device(\"cpu\")\n",
        "\n",
        "\n",
        "def bytes2human(n: int) -> str:\n",
        "    symbols = ('B','KB','MB','GB','TB','PB')\n",
        "    i = 0\n",
        "    n = float(n)\n",
        "    while n >= 1024 and i < len(symbols) - 1:\n",
        "        n /= 1024.0\n",
        "        i += 1\n",
        "    return f\"{n:.2f} {symbols[i]}\"\n",
        "\n",
        "\n",
        "# ----------------------------\n",
        "# Data\n",
        "# ----------------------------\n",
        "def get_data(num_workers: int = 5, batch_size: int = 64):\n",
        "    \"\"\"\n",
        "    Returns CIFAR-10 train and test dataloaders with given workers and batch size.\n",
        "    \"\"\"\n",
        "    tfm = transforms.Compose([\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(mean=(0.4914, 0.4822, 0.4465),\n",
        "                             std=(0.2470, 0.2435, 0.2616)),\n",
        "    ])\n",
        "    train_ds = datasets.CIFAR10(root=\"./data\", train=True, download=True, transform=tfm)\n",
        "    test_ds  = datasets.CIFAR10(root=\"./data\", train=False, download=True, transform=tfm)\n",
        "\n",
        "    dev = device_auto()\n",
        "    pin_memory = (dev.type == \"cuda\")\n",
        "    persistent_workers = (num_workers > 0)\n",
        "\n",
        "    train_dl = DataLoader(train_ds, batch_size=batch_size, shuffle=True,\n",
        "                          num_workers=num_workers, pin_memory=pin_memory,\n",
        "                          persistent_workers=persistent_workers, drop_last=True)\n",
        "    val_dl   = DataLoader(test_ds, batch_size=batch_size, shuffle=False,\n",
        "                          num_workers=num_workers, pin_memory=pin_memory,\n",
        "                          persistent_workers=persistent_workers, drop_last=False)\n",
        "    return train_dl, val_dl\n",
        "\n",
        "\n",
        "# ----------------------------\n",
        "# Evaluation\n",
        "# ----------------------------\n",
        "@torch.no_grad()\n",
        "def evaluate(model: nn.Module, val_dl: DataLoader, device: torch.device) -> float:\n",
        "    model.eval()\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    for images, labels in val_dl:\n",
        "        images = images.to(device, non_blocking=True)\n",
        "        labels = labels.to(device, non_blocking=True)\n",
        "        outputs = model(images)\n",
        "        preds = outputs.argmax(dim=1)\n",
        "        correct += (preds == labels).sum().item()\n",
        "        total += labels.size(0)\n",
        "    acc = 100.0 * correct / max(1, total)\n",
        "    return acc\n",
        "\n",
        "\n",
        "# ----------------------------\n",
        "# Training (epoch-based)\n",
        "# ----------------------------\n",
        "def train_one_epoch(model: nn.Module,\n",
        "                    train_dl: DataLoader,\n",
        "                    device: torch.device,\n",
        "                    optimizer: torch.optim.Optimizer,\n",
        "                    criterion: nn.Module,\n",
        "                    scaler: torch.amp.GradScaler,\n",
        "                    amp: bool = True,\n",
        "                    limit_train_batches: Optional[int] = None) -> Dict[str, Any]:\n",
        "    \"\"\"\n",
        "    Train for a single epoch. Optionally limit the number of batches processed\n",
        "    (useful for quick sweeps).\n",
        "    Returns a dict with time breakdowns and averages.\n",
        "    \"\"\"\n",
        "    model.train()\n",
        "\n",
        "    # Memory tracking\n",
        "    if device.type == \"cuda\":\n",
        "        torch.cuda.reset_peak_memory_stats()\n",
        "    proc = psutil.Process(os.getpid()) if psutil else None\n",
        "    peak_rss = proc.memory_info().rss if proc else 0\n",
        "\n",
        "    start_epoch = time.perf_counter()\n",
        "    loader_time = 0.0\n",
        "    compute_time = 0.0\n",
        "    loss_sum = 0.0\n",
        "    steps = 0\n",
        "    images_seen = 0\n",
        "\n",
        "    dl_iter = iter(train_dl)\n",
        "    batches_to_run = len(train_dl) if limit_train_batches is None else min(len(train_dl), limit_train_batches)\n",
        "\n",
        "    for _ in range(batches_to_run):\n",
        "        # Measure loader time\n",
        "        t1 = time.perf_counter()\n",
        "        try:\n",
        "            images, labels = next(dl_iter)\n",
        "        except StopIteration:\n",
        "            dl_iter = iter(train_dl)\n",
        "            images, labels = next(dl_iter)\n",
        "        t2 = time.perf_counter()\n",
        "\n",
        "        images = images.to(device, non_blocking=True)\n",
        "        labels = labels.to(device, non_blocking=True)\n",
        "\n",
        "        optimizer.zero_grad(set_to_none=True)\n",
        "\n",
        "        # Compute time\n",
        "        t3 = time.perf_counter()\n",
        "        use_amp = amp and device.type == \"cuda\"\n",
        "        with torch.amp.autocast(\"cuda\", enabled= use_amp):\n",
        "            outputs = model(images)\n",
        "            loss = criterion(outputs, labels)\n",
        "        if scaler.is_enabled():\n",
        "            scaler.scale(loss).backward()\n",
        "            scaler.step(optimizer)\n",
        "            scaler.update()\n",
        "        else:\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "        t4 = time.perf_counter()\n",
        "\n",
        "        loader_time += (t2 - t1)\n",
        "        compute_time += (t4 - t3)\n",
        "        loss_sum += loss.item()\n",
        "        steps += 1\n",
        "        images_seen += images.size(0)\n",
        "\n",
        "        if proc:\n",
        "            rss = proc.memory_info().rss\n",
        "            if rss > peak_rss:\n",
        "                peak_rss = rss\n",
        "\n",
        "    end_epoch = time.perf_counter()\n",
        "    epoch_time = end_epoch - start_epoch\n",
        "    avg_loss = loss_sum / max(1, steps)\n",
        "    throughput = images_seen / max(1e-9, epoch_time)\n",
        "\n",
        "    if device.type == \"cuda\":\n",
        "        peak_mem_bytes = torch.cuda.max_memory_allocated()\n",
        "        peak_mem_human = bytes2human(peak_mem_bytes)\n",
        "    else:\n",
        "        peak_mem_bytes = peak_rss\n",
        "        peak_mem_human = bytes2human(peak_mem_bytes)\n",
        "\n",
        "    return {\n",
        "        \"epoch_time\": epoch_time,\n",
        "        \"avg_loss\": avg_loss,\n",
        "        \"images_seen\": images_seen,\n",
        "        \"throughput\": throughput,\n",
        "        \"loader_ratio\": loader_time / max(1e-9, (loader_time + compute_time)),\n",
        "        \"compute_ratio\": compute_time / max(1e-9, (loader_time + compute_time)),\n",
        "        \"peak_mem_bytes\": peak_mem_bytes,\n",
        "        \"peak_mem_human\": peak_mem_human,\n",
        "    }\n",
        "\n",
        "\n",
        "def fit(model: nn.Module,\n",
        "        train_dl: DataLoader,\n",
        "        val_dl: DataLoader,\n",
        "        device: torch.device,\n",
        "        epochs: int = 1,\n",
        "        lr: float = 0.1,\n",
        "        amp: bool = True,\n",
        "        limit_train_batches: Optional[int] = None,\n",
        "        tb_writer: Optional[\"SummaryWriter\"] = None,\n",
        "        run_name: Optional[str] = None) -> Dict[str, Any]:\n",
        "    \"\"\"\n",
        "    Trains model for a few epochs and evaluates after each epoch.\n",
        "    Logs metrics to TensorBoard if writer is provided.\n",
        "    Returns per-epoch metrics and aggregates.\n",
        "    \"\"\"\n",
        "    model = model.to(device)\n",
        "    criterion = nn.CrossEntropyLoss().to(device)\n",
        "    optimizer = optim.SGD(model.parameters(), lr=lr, momentum=0.9, weight_decay=5e-4)\n",
        "    scaler = torch.amp.GradScaler(enabled=(amp and device.type == \"cuda\"))\n",
        "\n",
        "    epoch_stats = []\n",
        "\n",
        "    for epoch in range(1, epochs + 1):\n",
        "        tr = train_one_epoch(model, train_dl, device, optimizer, criterion, scaler,\n",
        "                             amp=amp, limit_train_batches=limit_train_batches)\n",
        "        val_acc = evaluate(model, val_dl, device)\n",
        "\n",
        "        # Log to TensorBoard\n",
        "        if tb_writer is not None:\n",
        "            tb_writer.add_scalar(\"train/avg_loss\", tr[\"avg_loss\"], epoch)\n",
        "            tb_writer.add_scalar(\"train/throughput_img_per_sec\", tr[\"throughput\"], epoch)\n",
        "            tb_writer.add_scalar(\"train/epoch_time_sec\", tr[\"epoch_time\"], epoch)\n",
        "            tb_writer.add_scalar(\"train/loader_time_pct\", tr[\"loader_ratio\"] * 100.0, epoch)\n",
        "            tb_writer.add_scalar(\"train/compute_time_pct\", tr[\"compute_ratio\"] * 100.0, epoch)\n",
        "            tb_writer.add_scalar(\"val/accuracy_pct\", val_acc, epoch)\n",
        "            # Log peak memory (unit depends on device)\n",
        "            if device.type == \"cuda\":\n",
        "                tb_writer.add_scalar(\"sys/peak_vram_GB\", tr[\"peak_mem_bytes\"] / (1024**3), epoch)\n",
        "            else:\n",
        "                tb_writer.add_scalar(\"sys/peak_rss_GB\", tr[\"peak_mem_bytes\"] / (1024**3), epoch)\n",
        "\n",
        "        epoch_stats.append({\n",
        "            \"epoch\": epoch,\n",
        "            **tr,\n",
        "            \"val_acc\": val_acc,\n",
        "        })\n",
        "\n",
        "    # Aggregates\n",
        "    avg_throughput = sum(e[\"throughput\"] for e in epoch_stats) / max(1, len(epoch_stats))\n",
        "    avg_epoch_time = sum(e[\"epoch_time\"] for e in epoch_stats) / max(1, len(epoch_stats))\n",
        "    final_val_acc = epoch_stats[-1][\"val_acc\"] if epoch_stats else 0.0\n",
        "\n",
        "    return {\n",
        "        \"epoch_stats\": epoch_stats,\n",
        "        \"avg_throughput\": avg_throughput,\n",
        "        \"avg_epoch_time\": avg_epoch_time,\n",
        "        \"final_val_acc\": final_val_acc,\n",
        "    }\n",
        "\n",
        "\n",
        "# ----------------------------\n",
        "# Sweep experiment\n",
        "# ----------------------------\n",
        "def sweep(\n",
        "    batch_sizes: List[int],\n",
        "    workers_list: List[int],\n",
        "    epochs: int = 1,\n",
        "    amp: bool = True,\n",
        "    limit_train_batches: Optional[int] = 200,  # set to None for full epochs\n",
        "    log_root: str = \"runs\",\n",
        "):\n",
        "    dev = device_auto()\n",
        "    print(f\"Using device: {dev}\")\n",
        "\n",
        "    if SummaryWriter is None:\n",
        "        print(\"âš ï¸  TensorBoard not found. Install with: pip install tensorboard\")\n",
        "    os.makedirs(log_root, exist_ok=True)\n",
        "\n",
        "    results: Dict[Tuple[int, int], Dict[str, Any]] = {}\n",
        "\n",
        "    for bs in batch_sizes:\n",
        "        for nw in workers_list:\n",
        "            run_name = f\"bs{bs}_w{nw}\"\n",
        "            print(f\"\\n=== Config: {run_name} ===\")\n",
        "            train_dl, val_dl = get_data(num_workers=nw, batch_size=bs)\n",
        "\n",
        "            # Initialize model fresh per config\n",
        "            model = ResNet50CIFAR()\n",
        "\n",
        "            # TensorBoard writer for this config\n",
        "            writer = SummaryWriter(log_dir=os.path.join(log_root, run_name)) if SummaryWriter else None\n",
        "            if writer is not None:\n",
        "                # (Optional) Log hparams summary\n",
        "                hparams = {\"batch_size\": bs, \"num_workers\": nw, \"amp\": int(bool(amp))}\n",
        "                writer.add_text(\"hparams\", str(hparams), global_step=0)\n",
        "\n",
        "            stats = fit(model, train_dl, val_dl, dev,\n",
        "                        epochs=epochs, lr=0.1, amp=amp,\n",
        "                        limit_train_batches=limit_train_batches,\n",
        "                        tb_writer=writer, run_name=run_name)\n",
        "\n",
        "            if writer is not None:\n",
        "                writer.flush()\n",
        "                writer.close()\n",
        "\n",
        "            print(f\"Avg throughput: {stats['avg_throughput']:.1f} img/s | \"\n",
        "                  f\"Avg epoch time: {stats['avg_epoch_time']:.2f}s | \"\n",
        "                  f\"Final val acc: {stats['final_val_acc']:.2f}%\")\n",
        "\n",
        "            results[(bs, nw)] = stats\n",
        "\n",
        "    # ---------------- Plotting ----------------\n",
        "    import numpy as np\n",
        "    bs_vals = batch_sizes\n",
        "    nw_vals = workers_list\n",
        "\n",
        "    # Heatmap: avg throughput\n",
        "    thr_heat = np.zeros((len(nw_vals), len(bs_vals)), dtype=float)\n",
        "    acc_heat = np.zeros((len(nw_vals), len(bs_vals)), dtype=float)\n",
        "    time_heat = np.zeros((len(nw_vals), len(bs_vals)), dtype=float)\n",
        "\n",
        "    for i, nw in enumerate(nw_vals):\n",
        "        for j, bs in enumerate(bs_vals):\n",
        "            r = results[(bs, nw)]\n",
        "            thr_heat[i, j] = r[\"avg_throughput\"]\n",
        "            acc_heat[i, j] = r[\"final_val_acc\"]\n",
        "            time_heat[i, j] = r[\"avg_epoch_time\"]\n",
        "\n",
        "    fig, axes = plt.subplots(1, 3, figsize=(20, 5))\n",
        "    # Throughput heatmap\n",
        "    ax = axes[0]\n",
        "    im0 = ax.imshow(thr_heat, cmap=\"viridis\", aspect=\"auto\", origin=\"lower\")\n",
        "    ax.set_title(\"Avg Throughput (images/sec)\")\n",
        "    ax.set_xticks(range(len(bs_vals))); ax.set_xticklabels([str(b) for b in bs_vals])\n",
        "    ax.set_xlabel(\"Batch size\")\n",
        "    ax.set_yticks(range(len(nw_vals))); ax.set_yticklabels([str(n) for n in nw_vals])\n",
        "    ax.set_ylabel(\"num_workers\")\n",
        "    cb0 = plt.colorbar(im0, ax=ax); cb0.set_label(\"images/sec\")\n",
        "\n",
        "    # Validation accuracy heatmap\n",
        "    ax = axes[1]\n",
        "    im1 = ax.imshow(acc_heat, cmap=\"magma\", aspect=\"auto\", origin=\"lower\", vmin=0, vmax=100)\n",
        "    ax.set_title(\"Final Validation Accuracy (%)\")\n",
        "    ax.set_xticks(range(len(bs_vals))); ax.set_xticklabels([str(b) for b in bs_vals])\n",
        "    ax.set_xlabel(\"Batch size\")\n",
        "    ax.set_yticks(range(len(nw_vals))); ax.set_yticklabels([str(n) for n in nw_vals])\n",
        "    ax.set_ylabel(\"num_workers\")\n",
        "    cb1 = plt.colorbar(im1, ax=ax); cb1.set_label(\"%\")\n",
        "\n",
        "    # Time per epoch heatmap (seconds)\n",
        "    ax = axes[2]\n",
        "    im2 = ax.imshow(time_heat, cmap=\"plasma\", aspect=\"auto\", origin=\"lower\")\n",
        "    ax.set_title(\"Avg Time per Epoch (sec)\")\n",
        "    ax.set_xticks(range(len(bs_vals))); ax.set_xticklabels([str(b) for b in bs_vals])\n",
        "    ax.set_xlabel(\"Batch size\")\n",
        "    ax.set_yticks(range(len(nw_vals))); ax.set_yticklabels([str(n) for n in nw_vals])\n",
        "    ax.set_ylabel(\"num_workers\")\n",
        "    cb2 = plt.colorbar(im2, ax=ax); cb2.set_label(\"sec\")\n",
        "\n",
        "    plt.tight_layout()\n",
        "    out = \"workers_batchsize_tb_summary.png\"\n",
        "    plt.savefig(out, bbox_inches=\"tight\")\n",
        "    print(f\"\\nSaved figure to {out}\")\n",
        "    plt.show()\n",
        "\n",
        "    return results\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    set_seed(123)\n",
        "\n",
        "    # Keep the grid small to start; expand once it's working well on your machine.\n",
        "    batch_sizes = [32, 64, 128]\n",
        "    workers_list = [0, 2, 4, 8]\n",
        "\n",
        "    # Epochs: start with 1â€“2 for quick tests. Increase for stability.\n",
        "    results = sweep(\n",
        "        batch_sizes=batch_sizes,\n",
        "        workers_list=workers_list,\n",
        "        epochs=2,\n",
        "        amp=True,                   # set False if you want to compare without AMP\n",
        "        limit_train_batches=200,    # set to None for full epochs over the entire train set\n",
        "        log_root=\"runs\",            # TensorBoard root\n",
        "    )"
      ],
      "metadata": {
        "id": "9xRjB_TActfT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Analysis:**\n",
        "\n",
        "1ï¸âƒ£ What batch_size stresses in the system\n",
        "\n",
        "**Primary impact**\n",
        "    - Compute units (GPU SMs / CPU vector units)\n",
        "    - Device memory (VRAM / RAM)\n",
        "    - Kernel efficiency\n",
        "\n",
        "**Behavior**\n",
        "  - Increasing batch size:\n",
        "    - Improves compute utilization\n",
        "    - Reduces perâ€‘sample overhead\n",
        "    - Increases activation + gradient memory\n",
        "\n",
        "\n",
        "**Once compute is saturated:**\n",
        "    - Larger batches do not increase throughput\n",
        "    - Only increase memory usage and latency\n",
        "**System bottleneck triggered by large batch**\n",
        "\n",
        "2ï¸âƒ£ What num_workers stresses in the system\n",
        "**Primary impact**\n",
        "  - CPU cores\n",
        "  - System RAM\n",
        "  - Disk / filesystem\n",
        "  - Inter-process communication\n",
        "\n",
        "**Behavior**\n",
        "  - Increasing num_workers:\n",
        "    - Parallelizes data loading & augmentation\n",
        "    - Reduces GPU idle time\n",
        "**Too many workers:**\n",
        "    - CPU oversubscription\n",
        "    - Cache thrashing\n",
        "    - Context-switch overhead\n",
        "    - Memory pressure\n",
        "\n",
        "âœ… **Key Takeaways**\n",
        "    - Batch size controls compute efficiency\n",
        "    - Workers control data pipeline efficiency\n",
        "    - The system improves only when both pipelines are balanced\n",
        "    - More is not better once a bottleneck is reached\n",
        "\n",
        "**Optimal settings are hardware and workload-specific**\n"
      ],
      "metadata": {
        "id": "12rJSCUWoJdX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "==========================================================================="
      ],
      "metadata": {
        "id": "0L6oiFBKUcc3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Part-2 : Blockchain-based Data Parallelism"
      ],
      "metadata": {
        "id": "nRrcB_z7QTMI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1. Imports & Utilities (Hashing, JSON, HMAC)"
      ],
      "metadata": {
        "id": "Mh6LB85yQk39"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "#@title ðŸ” Canonical JSON, Hashing, HMAC Signer\n",
        "import hashlib, hmac, json, os, time, math, random\n",
        "from dataclasses import dataclass, asdict, field\n",
        "from typing import List, Dict, Optional, Tuple\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def json_dumps_canonical(obj) -> str:\n",
        "    return json.dumps(obj, sort_keys=True, separators=(\",\", \":\"))\n",
        "\n",
        "def sha256_hex(data: bytes) -> str:\n",
        "    return hashlib.sha256(data).hexdigest()\n",
        "\n",
        "def tensor_sha256_hex(tensor) -> str:\n",
        "    return sha256_hex(tensor.detach().cpu().numpy().tobytes())\n",
        "\n",
        "class HMACSigner:\n",
        "    \"\"\"Demo signer; replace with Ed25519 for production.\"\"\"\n",
        "    def __init__(self, signer_id: str, key: Optional[bytes] = None):\n",
        "        self.signer_id = signer_id\n",
        "        self.key = key or os.urandom(32)\n",
        "\n",
        "    def sign(self, message: bytes) -> str:\n",
        "        return hmac.new(self.key, message, hashlib.sha256).hexdigest()\n",
        "\n",
        "    def verify(self, message: bytes, signature: str) -> bool:\n",
        "        expected = self.sign(message)\n",
        "        return hmac.compare_digest(expected, signature)\n"
      ],
      "metadata": {
        "id": "aLmfn0mUQdcX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2) CIFARâ€‘10 Data, Model, Backdoor Trigger, Metrics"
      ],
      "metadata": {
        "id": "QRsBvdJ-Q6aK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.utils.data as data\n",
        "import torchvision\n",
        "import torchvision.transforms as T\n",
        "import torch.backends.cudnn as cudnn\n",
        "\n",
        "class SmallCifarCNN(nn.Module):\n",
        "    def __init__(self, num_classes: int = 10):\n",
        "        super().__init__()\n",
        "        self.features = nn.Sequential(\n",
        "            nn.Conv2d(3, 32, 3, padding=1), nn.BatchNorm2d(32), nn.ReLU(True),\n",
        "            nn.Conv2d(32, 32, 3, padding=1), nn.BatchNorm2d(32), nn.ReLU(True),\n",
        "            nn.MaxPool2d(2),  # 16x16\n",
        "            nn.Conv2d(32, 64, 3, padding=1), nn.BatchNorm2d(64), nn.ReLU(True),\n",
        "            nn.Conv2d(64, 64, 3, padding=1), nn.BatchNorm2d(64), nn.ReLU(True),\n",
        "            nn.MaxPool2d(2),  # 8x8\n",
        "            nn.Conv2d(64, 128, 3, padding=1), nn.BatchNorm2d(128), nn.ReLU(True),\n",
        "            nn.Conv2d(128, 128, 3, padding=1), nn.BatchNorm2d(128), nn.ReLU(True),\n",
        "            nn.AdaptiveAvgPool2d((1,1))\n",
        "        )\n",
        "        self.classifier = nn.Linear(128, num_classes)\n",
        "    def forward(self, x): return self.classifier(self.features(x).view(x.size(0), -1))\n",
        "\n",
        "def make_cifar10_datasets(data_dir=\"./data\"):\n",
        "    mean = (0.4914, 0.4822, 0.4465)\n",
        "    std  = (0.2470, 0.2435, 0.2616)\n",
        "    train_tfms = T.Compose([\n",
        "        T.RandomCrop(32, padding=4),\n",
        "        T.RandomHorizontalFlip(),\n",
        "        T.ToTensor(),\n",
        "        T.Normalize(mean, std),\n",
        "    ])\n",
        "    test_tfms = T.Compose([\n",
        "        T.ToTensor(),\n",
        "        T.Normalize(mean, std),\n",
        "    ])\n",
        "    train_ds = torchvision.datasets.CIFAR10(root=data_dir, train=True, download=True, transform=train_tfms)\n",
        "    test_ds  = torchvision.datasets.CIFAR10(root=data_dir, train=False, download=True, transform=test_tfms)\n",
        "    return train_ds, test_ds, mean, std\n",
        "\n",
        "def overlay_trigger(img_tensor: torch.Tensor, size: int = 4, value: float = 2.5):\n",
        "    c, h, w = img_tensor.shape\n",
        "    img = img_tensor.clone()\n",
        "    img[:, h-size:h, w-size:w] = value  # bright square in normalized space\n",
        "    return img\n",
        "\n",
        "class PoisonedWrapper(data.Dataset):\n",
        "    def __init__(self, base_ds: data.Dataset, poison_rate: float, target_label: int, trigger_size: int = 4):\n",
        "        self.base = base_ds\n",
        "        self.poison_rate = poison_rate\n",
        "        self.target_label = target_label\n",
        "        self.trigger_size = trigger_size\n",
        "    def __len__(self): return len(self.base)\n",
        "    def __getitem__(self, idx):\n",
        "        x, y = self.base[idx]\n",
        "        if random.random() < self.poison_rate:\n",
        "            x = overlay_trigger(x, size=self.trigger_size)\n",
        "            y = self.target_label\n",
        "        return x, y\n",
        "\n",
        "@torch.no_grad()\n",
        "def accuracy_top1(model: nn.Module, dl: data.DataLoader, device: torch.device) -> float:\n",
        "    model.eval(); correct = 0; total = 0\n",
        "    for xb, yb in dl:\n",
        "        xb, yb = xb.to(device), yb.to(device)\n",
        "        logits = model(xb)\n",
        "        preds = logits.argmax(dim=1)\n",
        "        correct += (preds == yb).sum().item(); total += yb.numel()\n",
        "    return correct / max(1, total)\n",
        "\n",
        "@torch.no_grad()\n",
        "def asr_backdoor(model: nn.Module, clean_test_ds: data.Dataset, target_label: int,\n",
        "                 device: torch.device, trigger_size: int = 4, batch: int = 512):\n",
        "    class Triggered(data.Dataset):\n",
        "        def __init__(self, base): self.base = base\n",
        "        def __len__(self): return len(self.base)\n",
        "        def __getitem__(self, i):\n",
        "            x, y = self.base[i]\n",
        "            x = overlay_trigger(x, size=trigger_size)\n",
        "            return x, target_label\n",
        "    dl = data.DataLoader(Triggered(clean_test_ds), batch_size=batch, shuffle=False, num_workers=2)\n",
        "    model.eval(); hit = 0; total = 0\n",
        "    for xb, _ in dl:\n",
        "        xb = xb.to(device)\n",
        "        logits = model(xb)\n",
        "        preds = logits.argmax(dim=1).cpu()\n",
        "        hit += (preds == target_label).sum().item()\n",
        "        total += preds.numel()\n",
        "    return hit / max(1, total)\n",
        "\n",
        "def model_hash(model: nn.Module) -> str:\n",
        "    h = hashlib.sha256()\n",
        "    with torch.no_grad():\n",
        "        for k, v in model.state_dict().items():\n",
        "            h.update(k.encode()); h.update(v.detach().cpu().numpy().tobytes())\n",
        "    return h.hexdigest()\n",
        "\n",
        "def count_params(model: nn.Module) -> int:\n",
        "    return sum(p.numel() for p in model.parameters())"
      ],
      "metadata": {
        "id": "d4iHhqnPQ5_1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3) Vectorization & Robust Aggregation (for FL)"
      ],
      "metadata": {
        "id": "GPh138vuRDP7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ModelVectorHelper:\n",
        "    def __init__(self, model: nn.Module, device: torch.device):\n",
        "        self.device = device\n",
        "        self.param_shapes, self.param_numels = [], []\n",
        "        for p in model.parameters():\n",
        "            self.param_shapes.append(tuple(p.shape))\n",
        "            self.param_numels.append(p.numel())\n",
        "        self.total_numel = sum(self.param_numels)\n",
        "\n",
        "    def grads_to_vector(self, model: nn.Module) -> torch.Tensor:\n",
        "        chunks = []\n",
        "        for p, n in zip(model.parameters(), self.param_numels):\n",
        "            chunks.append(torch.zeros(n, device=self.device) if p.grad is None\n",
        "                          else p.grad.detach().reshape(-1).to(self.device))\n",
        "        return torch.cat(chunks, dim=0)\n",
        "\n",
        "    def apply_update(self, model: nn.Module, delta_vec: torch.Tensor, lr: float):\n",
        "        assert delta_vec.numel() == self.total_numel\n",
        "        off = 0\n",
        "        with torch.no_grad():\n",
        "            for p, n in zip(model.parameters(), self.param_numels):\n",
        "                g = delta_vec[off:off+n].view_as(p).to(p.device)\n",
        "                p.add_(g, alpha=-lr)\n",
        "                off += n\n",
        "\n",
        "def l2_norm(x: torch.Tensor) -> float:\n",
        "    return float(torch.linalg.vector_norm(x).item())\n",
        "\n",
        "def clip_by_l2(x: torch.Tensor, max_norm: float) -> torch.Tensor:\n",
        "    n = torch.linalg.vector_norm(x)\n",
        "    return x if (n <= 0 or n <= max_norm) else x * (max_norm / n)\n",
        "\n",
        "def coord_median(updates: List[torch.Tensor]) -> torch.Tensor:\n",
        "    M = torch.stack(updates, dim=0)\n",
        "    return M.median(dim=0).values\n",
        "\n",
        "def trimmed_mean(updates: List[torch.Tensor], trim_ratio: float = 0.1) -> torch.Tensor:\n",
        "    M = torch.stack(updates, dim=0)\n",
        "    n = M.size(0); k = int(n * trim_ratio)\n",
        "    if 2*k >= n: return M.mean(dim=0)\n",
        "    sorted_vals, _ = torch.sort(M, dim=0)\n",
        "    return sorted_vals[k:n-k, :].mean(dim=0)"
      ],
      "metadata": {
        "id": "nHR77KHERI_q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 4)Single-node training with metrics"
      ],
      "metadata": {
        "id": "vnZlpnEVRNou"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train_single_node(\n",
        "    epochs=2, batch_size=128, lr=0.1, seed=123, use_cuda=True, num_workers_dl=2\n",
        "):\n",
        "    torch.manual_seed(seed); np.random.seed(seed); random.seed(seed); cudnn.benchmark = True\n",
        "    device = torch.device(\"cuda\") if (use_cuda and torch.cuda.is_available()) else torch.device(\"cpu\")\n",
        "\n",
        "    train_ds, test_ds, _, _ = make_cifar10_datasets()\n",
        "    train_loader = data.DataLoader(train_ds, batch_size=batch_size, shuffle=True, num_workers=num_workers_dl, pin_memory=True)\n",
        "    test_loader  = data.DataLoader(test_ds, batch_size=512, shuffle=False, num_workers=num_workers_dl, pin_memory=True)\n",
        "\n",
        "    model = SmallCifarCNN().to(device)\n",
        "    opt = torch.optim.SGD(model.parameters(), lr=lr, momentum=0.9, weight_decay=5e-4)\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "    epoch_times, epoch_throughputs, acc_curve = [], [], []\n",
        "    samples_per_epoch = len(train_loader.dataset)\n",
        "\n",
        "    for ep in range(1, epochs+1):\n",
        "        model.train()\n",
        "        start = time.time()\n",
        "        seen = 0\n",
        "        for xb, yb in train_loader:\n",
        "            xb, yb = xb.to(device), yb.to(device)\n",
        "            opt.zero_grad(set_to_none=True)\n",
        "            logits = model(xb)\n",
        "            loss = criterion(logits, yb)\n",
        "            loss.backward()\n",
        "            opt.step()\n",
        "            seen += yb.size(0)\n",
        "        elapsed = time.time() - start\n",
        "        epoch_times.append(elapsed)\n",
        "        epoch_throughputs.append(seen / elapsed)\n",
        "        acc = accuracy_top1(model, test_loader, device=device)\n",
        "        acc_curve.append(acc)\n",
        "        print(f\"[Single] Epoch {ep}/{epochs} | time={elapsed:.2f}s | thrpt={seen/elapsed:.1f} samp/s | acc={acc:.4f}\")\n",
        "\n",
        "    return {\n",
        "        \"model\": model, \"device\": device,\n",
        "        \"epoch_times\": epoch_times,\n",
        "        \"epoch_throughputs\": epoch_throughputs,\n",
        "        \"acc_curve\": acc_curve,\n",
        "        \"samples_per_epoch\": samples_per_epoch\n",
        "    }"
      ],
      "metadata": {
        "id": "FVFDPVzORTzz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 5) Pipeline Parallelism (Microâ€‘batching, Bubble Overhead)"
      ],
      "metadata": {
        "id": "S0NTNgI6RZna"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class TwoStageCifar(nn.Module):\n",
        "    \"\"\"Split SmallCifarCNN into features (stage1) and classifier (stage2).\"\"\"\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        base = SmallCifarCNN()\n",
        "        self.stage1 = base.features\n",
        "        self.stage2 = base.classifier\n",
        "    def forward(self, x):\n",
        "        x = self.stage1(x); x = x.view(x.size(0), -1); return self.stage2(x)\n",
        "\n",
        "def train_pipeline(\n",
        "    epochs=2, batch_size=128, micro_batches=4, lr=0.1, seed=123, use_cuda=True, num_workers_dl=2\n",
        "):\n",
        "    assert batch_size % micro_batches == 0, \"batch_size must be divisible by micro_batches\"\n",
        "    torch.manual_seed(seed); np.random.seed(seed); random.seed(seed); cudnn.benchmark = True\n",
        "    device = torch.device(\"cuda\") if (use_cuda and torch.cuda.is_available()) else torch.device(\"cpu\")\n",
        "\n",
        "    train_ds, test_ds, _, _ = make_cifar10_datasets()\n",
        "    train_loader = data.DataLoader(train_ds, batch_size=batch_size, shuffle=True, num_workers=num_workers_dl, pin_memory=True)\n",
        "    test_loader  = data.DataLoader(test_ds, batch_size=512, shuffle=False, num_workers=num_workers_dl, pin_memory=True)\n",
        "\n",
        "    model = TwoStageCifar().to(device)\n",
        "    opt = torch.optim.SGD(model.parameters(), lr=lr, momentum=0.9, weight_decay=5e-4)\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "    epoch_times, epoch_throughputs, acc_curve = [], [], []\n",
        "    mb_size = batch_size // micro_batches\n",
        "    S = 2; M = micro_batches\n",
        "    theoretical_bubble = (S - 1) / (M + S - 1)\n",
        "\n",
        "    for ep in range(1, epochs+1):\n",
        "        model.train()\n",
        "        start = time.time()\n",
        "        seen = 0\n",
        "        for xb, yb in train_loader:\n",
        "            xb, yb = xb.to(device), yb.to(device)\n",
        "            opt.zero_grad(set_to_none=True)\n",
        "            # split into micro-batches\n",
        "            xb_chunks = xb.split(mb_size, dim=0)\n",
        "            yb_chunks = yb.split(mb_size, dim=0)\n",
        "            losses = []\n",
        "            # simple sequential pipeline (single GPU) for clarity\n",
        "            for xmb, ymb in zip(xb_chunks, yb_chunks):\n",
        "                z1 = model.stage1(xmb)\n",
        "                z1 = z1.view(z1.size(0), -1)\n",
        "                logits = model.stage2(z1)\n",
        "                loss = criterion(logits, ymb) / micro_batches  # scale loss to keep overall grad magnitude\n",
        "                loss.backward()\n",
        "                losses.append(loss.item())\n",
        "            opt.step()\n",
        "            seen += yb.size(0)\n",
        "        elapsed = time.time() - start\n",
        "        epoch_times.append(elapsed)\n",
        "        epoch_throughputs.append(seen / elapsed)\n",
        "        acc = accuracy_top1(model, test_loader, device=device)\n",
        "        acc_curve.append(acc)\n",
        "        print(f\"[Pipe] Epoch {ep}/{epochs} | time={elapsed:.2f}s | thrpt={seen/elapsed:.1f} | acc={acc:.4f} | bubbleâ‰ˆ{theoretical_bubble:.3f}\")\n",
        "\n",
        "    return {\n",
        "        \"model\": model, \"device\": device,\n",
        "        \"epoch_times\": epoch_times,\n",
        "        \"epoch_throughputs\": epoch_throughputs,\n",
        "        \"acc_curve\": acc_curve,\n",
        "        \"theoretical_bubble\": theoretical_bubble\n",
        "    }"
      ],
      "metadata": {
        "id": "1uB9i6ZpRcQs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 6) Federated worker, attacks, and FL runner with metrics"
      ],
      "metadata": {
        "id": "q_9Ts8HARkC0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "@dataclass\n",
        "class AttackProfile:\n",
        "    kind: str = \"none\"    # \"none\" | \"zero\" | \"mean_shift\" | \"gaussian\" | \"backdoor\" | \"label_flip\"\n",
        "    # gradient-space attacks\n",
        "    mean_shift_value: float = 5.0\n",
        "    gaussian_scale: float = 10.0\n",
        "    # backdoor params\n",
        "    poison_rate: float = 0.2\n",
        "    target_label: int = 0\n",
        "    trigger_size: int = 4\n",
        "    # label flip params\n",
        "    flip_from: Optional[int] = None\n",
        "    flip_to: Optional[int] = None\n",
        "\n",
        "class LabelFlipWrapper(data.Dataset):\n",
        "    def __init__(self, base: data.Dataset, flip_from: int, flip_to: int):\n",
        "        self.base, self.flip_from, self.flip_to = base, flip_from, flip_to\n",
        "    def __len__(self): return len(self.base)\n",
        "    def __getitem__(self, i):\n",
        "        x, y = self.base[i]\n",
        "        if y == self.flip_from: y = self.flip_to\n",
        "        return x, y\n",
        "\n",
        "class Worker:\n",
        "    def __init__(self, worker_id: str, dataset: data.Dataset, batch_size: int,\n",
        "                 device: torch.device, attack: AttackProfile, num_workers: int = 2):\n",
        "        # wrap dataset if needed\n",
        "        ds = dataset\n",
        "        if attack.kind == \"backdoor\":\n",
        "            ds = PoisonedWrapper(ds, poison_rate=attack.poison_rate, target_label=attack.target_label, trigger_size=attack.trigger_size)\n",
        "        if attack.kind == \"label_flip\" and attack.flip_from is not None and attack.flip_to is not None:\n",
        "            ds = LabelFlipWrapper(ds, attack.flip_from, attack.flip_to)\n",
        "\n",
        "        self.worker_id = worker_id\n",
        "        self.dataset = ds\n",
        "        self.attack = attack\n",
        "        self.device = device\n",
        "        self.loader = data.DataLoader(ds, batch_size=batch_size, shuffle=True, drop_last=False,\n",
        "                                      num_workers=num_workers, pin_memory=True)\n",
        "\n",
        "    def compute_avg_grad(self, model_fn, global_state_dict, vec_helper: ModelVectorHelper) -> Tuple[torch.Tensor, int]:\n",
        "        model = model_fn().to(self.device); model.load_state_dict(global_state_dict, strict=True); model.train()\n",
        "        criterion = nn.CrossEntropyLoss()\n",
        "        avg_grad = torch.zeros(vec_helper.total_numel, device=self.device); total = 0\n",
        "        for xb, yb in self.loader:\n",
        "            xb, yb = xb.to(self.device, non_blocking=True), yb.to(self.device, non_blocking=True)\n",
        "            for p in model.parameters():\n",
        "                if p.grad is not None: p.grad.zero_()\n",
        "            logits = model(xb); loss = criterion(logits, yb); loss.backward()\n",
        "            g = vec_helper.grads_to_vector(model)\n",
        "            bs = yb.numel(); avg_grad += g * bs; total += bs\n",
        "        if total > 0: avg_grad /= float(total)\n",
        "\n",
        "        # apply gradient-space attacks\n",
        "        if self.attack.kind == \"zero\":\n",
        "            avg_grad = torch.zeros_like(avg_grad)\n",
        "        elif self.attack.kind == \"mean_shift\":\n",
        "            avg_grad = avg_grad + self.attack.mean_shift_value\n",
        "        elif self.attack.kind == \"gaussian\":\n",
        "            avg_grad = torch.randn_like(avg_grad) * self.attack.gaussian_scale\n",
        "\n",
        "        return avg_grad.detach(), total\n",
        "\n",
        "def shard_indices(total: int, n_clients: int, per_client: int, seed: int = 123):\n",
        "    rng = np.random.default_rng(seed)\n",
        "    idx = np.arange(total); rng.shuffle(idx)\n",
        "    idx = idx[:n_clients * per_client]\n",
        "    return [idx[i*per_client:(i+1)*per_client] for i in range(n_clients)]\n",
        "\n",
        "def run_federated(\n",
        "    n_clients=6, per_client=6000, rounds=3, batch_size=128, lr=0.3, max_norm=5.0,\n",
        "    aggregator=\"coord_median\", trim_ratio=0.1,\n",
        "    malicious_fraction=0.33,\n",
        "    attacks_order=(\"zero\",\"mean_shift\",\"gaussian\"),  # cycle through kinds for malicious clients\n",
        "    backdoor_on_first=False, target_label=0, poison_rate=0.2,\n",
        "    label_flip=False, flip_from=1, flip_to=7,\n",
        "    seed=123, use_cuda=True, num_workers_dl=2\n",
        "):\n",
        "    torch.manual_seed(seed); np.random.seed(seed); random.seed(seed); cudnn.benchmark = True\n",
        "    device = torch.device(\"cuda\") if (use_cuda and torch.cuda.is_available()) else torch.device(\"cpu\")\n",
        "\n",
        "    # Data\n",
        "    train_ds, test_ds, _, _ = make_cifar10_datasets()\n",
        "    test_loader = data.DataLoader(test_ds, batch_size=512, shuffle=False, num_workers=num_workers_dl, pin_memory=True)\n",
        "\n",
        "    # Shard data\n",
        "    assert n_clients * per_client <= len(train_ds), \"Increase per_client or reduce n_clients\"\n",
        "    shards = shard_indices(len(train_ds), n_clients, per_client, seed=seed)\n",
        "\n",
        "    # Model factory + helper\n",
        "    def model_fn(): return SmallCifarCNN()\n",
        "    global_model = model_fn().to(device)\n",
        "    vec_helper = ModelVectorHelper(global_model, device=device)\n",
        "\n",
        "    # Workers + attacks\n",
        "    n_mal = max(0, int(n_clients * malicious_fraction))\n",
        "    workers = []\n",
        "    for i in range(n_clients):\n",
        "        subset = data.Subset(train_ds, shards[i])\n",
        "        if i < n_mal:\n",
        "            kind = attacks_order[i % len(attacks_order)]\n",
        "            if backdoor_on_first and i == 0:\n",
        "                attack = AttackProfile(kind=\"backdoor\", poison_rate=poison_rate, target_label=target_label, trigger_size=4)\n",
        "            elif label_flip and i == 1:\n",
        "                attack = AttackProfile(kind=\"label_flip\", flip_from=flip_from, flip_to=flip_to)\n",
        "            else:\n",
        "                if kind == \"zero\": attack = AttackProfile(kind=\"zero\")\n",
        "                elif kind == \"mean_shift\": attack = AttackProfile(kind=\"mean_shift\", mean_shift_value=5.0)\n",
        "                elif kind == \"gaussian\": attack = AttackProfile(kind=\"gaussian\", gaussian_scale=10.0)\n",
        "                else: attack = AttackProfile(kind=\"none\")\n",
        "        else:\n",
        "            attack = AttackProfile(kind=\"none\")\n",
        "        workers.append(Worker(f\"client-{i}\", subset, batch_size=batch_size, device=device, attack=attack, num_workers=num_workers_dl))\n",
        "\n",
        "    # Metrics\n",
        "    round_times, round_throughputs, acc_hist, asr_hist = [], [], [], []\n",
        "    reject_ratio_hist, grad_var_hist, comm_cost_mb_hist = [], [], []\n",
        "    grad_scatter_round = None  # (means, stds, is_mal) recorded at round 1\n",
        "\n",
        "    # Eval before\n",
        "    base_acc = accuracy_top1(global_model, test_loader, device=device)\n",
        "    base_asr = asr_backdoor(global_model, test_ds, target_label=target_label, device=device, trigger_size=4)\n",
        "    print(f\"[FL Init] acc={base_acc:.4f} | ASR={base_asr:.4f}\")\n",
        "\n",
        "    for r in range(1, rounds+1):\n",
        "        t0 = time.time()\n",
        "        global_state = {k: v.detach().cpu().clone() for k, v in global_model.state_dict().items()}\n",
        "\n",
        "        # Each client computes its update\n",
        "        grads = []\n",
        "        is_mal_flags = []\n",
        "        total_samples = 0\n",
        "        clipped_count = 0\n",
        "        for i, w in enumerate(workers):\n",
        "            g, n = w.compute_avg_grad(model_fn, global_state, vec_helper)\n",
        "            total_samples += n\n",
        "            # record stats for scatter at round 1\n",
        "            if r == 1:\n",
        "                g_cpu = g.cpu()\n",
        "                mean = float(g_cpu.mean().item()); std = float(g_cpu.std(unbiased=False).item())\n",
        "                if grad_scatter_round is None: grad_scatter_round = {\"mean\": [], \"std\": [], \"is_mal\": []}\n",
        "                grad_scatter_round[\"mean\"].append(mean); grad_scatter_round[\"std\"].append(std)\n",
        "                grad_scatter_round[\"is_mal\"].append(w.attack.kind != \"none\")\n",
        "            # clip\n",
        "            before = g.clone()\n",
        "            g = clip_by_l2(g, max_norm=max_norm)\n",
        "            if not torch.allclose(g, before):\n",
        "                clipped_count += 1\n",
        "            grads.append(g)\n",
        "            is_mal_flags.append(w.attack.kind != \"none\")\n",
        "\n",
        "        # Communication cost (if we transmitted raw gradients): size in MB\n",
        "        # float32 per param\n",
        "        d = grads[0].numel()\n",
        "        comm_cost_bytes = d * 4 * len(grads)\n",
        "        comm_cost_mb = comm_cost_bytes / (1024*1024)\n",
        "        comm_cost_mb_hist.append(comm_cost_mb)\n",
        "\n",
        "        # Robust aggregate\n",
        "        if aggregator == \"coord_median\":\n",
        "            agg = coord_median(grads)\n",
        "        elif aggregator == \"trimmed_mean\":\n",
        "            agg = trimmed_mean(grads, trim_ratio=trim_ratio)\n",
        "        else:\n",
        "            raise ValueError(\"aggregator must be 'coord_median' or 'trimmed_mean'\")\n",
        "\n",
        "        # Gradient variance (aggregate)\n",
        "        grad_var_hist.append(float(agg.var(unbiased=False).item()))\n",
        "\n",
        "        # Reject ratio proxy: % clipped (in absence of explicit filtering)\n",
        "        reject_ratio_hist.append(clipped_count / max(1, len(grads)))\n",
        "\n",
        "        # Apply update\n",
        "        vec_helper.apply_update(global_model, agg.to(torch.float32), lr=lr)\n",
        "\n",
        "        elapsed = time.time() - t0\n",
        "        round_times.append(elapsed)\n",
        "        round_throughputs.append(total_samples / elapsed)\n",
        "\n",
        "        # Evaluate\n",
        "        acc = accuracy_top1(global_model, test_loader, device=device)\n",
        "        asr = asr_backdoor(global_model, test_ds, target_label=target_label, device=device, trigger_size=4)\n",
        "        acc_hist.append(acc); asr_hist.append(asr)\n",
        "\n",
        "        print(f\"[FL] Round {r}/{rounds} | time={elapsed:.2f}s | thrpt={total_samples/elapsed:.1f} samp/s | acc={acc:.4f} | ASR={asr:.4f} | \"\n",
        "              f\"clip%={reject_ratio_hist[-1]*100:.1f} | gradVar={grad_var_hist[-1]:.2e} | commâ‰ˆ{comm_cost_mb:.1f} MB\")\n",
        "\n",
        "    return {\n",
        "        \"model\": global_model, \"device\": device,\n",
        "        \"round_times\": round_times,\n",
        "        \"round_throughputs\": round_throughputs,\n",
        "        \"acc_hist\": acc_hist,\n",
        "        \"asr_hist\": asr_hist,\n",
        "        \"reject_ratio_hist\": reject_ratio_hist,\n",
        "        \"grad_var_hist\": grad_var_hist,\n",
        "        \"comm_cost_mb_hist\": comm_cost_mb_hist,\n",
        "        \"grad_scatter_round\": grad_scatter_round,  # dict at round 1: mean/std/is_mal\n",
        "        \"n_clients\": n_clients\n",
        "    }"
      ],
      "metadata": {
        "id": "OGxwO2mURmnJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Run All Three Modes & Plot Required Metrics"
      ],
      "metadata": {
        "id": "c6jJRyCGRsk7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "SINGLE_EPOCHS = 2\n",
        "PIPE_EPOCHS   = 2\n",
        "FL_ROUNDS     = 3\n",
        "\n",
        "single_log = train_single_node(epochs=SINGLE_EPOCHS, batch_size=128, lr=0.1, use_cuda=True)\n",
        "pipe_log   = train_pipeline(epochs=PIPE_EPOCHS, batch_size=128, micro_batches=4, lr=0.1, use_cuda=True)\n",
        "fl_log     = run_federated(\n",
        "    n_clients=6, per_client=6000, rounds=FL_ROUNDS, batch_size=128, lr=0.3,\n",
        "    max_norm=5.0, aggregator=\"coord_median\",\n",
        "    malicious_fraction=0.33, attacks_order=(\"zero\",\"mean_shift\",\"gaussian\"),\n",
        "    backdoor_on_first=True, target_label=0, poison_rate=0.2,\n",
        "    label_flip=False, use_cuda=True\n",
        ")"
      ],
      "metadata": {
        "id": "Y75Q9r6ORyme"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# A) Performance: Time, Throughput, Speedup, Efficiency"
      ],
      "metadata": {
        "id": "gQ-CSu3QR7ME"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "#@title â±ï¸ Time, ðŸ“ˆ Throughput, âš¡ Speedup & Efficiency\n",
        "def plot_perf(single_log, pipe_log, fl_log, n_stages_pipeline=2, n_clients_fl=None):\n",
        "    n_clients_fl = n_clients_fl or fl_log[\"n_clients\"]\n",
        "\n",
        "    # Collect stats\n",
        "    t_single = np.array(single_log[\"epoch_times\"])\n",
        "    thr_single = np.array(single_log[\"epoch_throughputs\"])\n",
        "    t_pipe = np.array(pipe_log[\"epoch_times\"])\n",
        "    thr_pipe = np.array(pipe_log[\"epoch_throughputs\"])\n",
        "    t_fl = np.array(fl_log[\"round_times\"])\n",
        "    thr_fl = np.array(fl_log[\"round_throughputs\"])\n",
        "\n",
        "    # Aggregate means\n",
        "    m_single_t = t_single.mean(); m_pipe_t = t_pipe.mean(); m_fl_t = t_fl.mean()\n",
        "    m_single_thr = thr_single.mean(); m_pipe_thr = thr_pipe.mean(); m_fl_thr = thr_fl.mean()\n",
        "\n",
        "    # Speedup vs single-node (use throughput)\n",
        "    speedup_pipe = m_pipe_thr / m_single_thr\n",
        "    speedup_fl   = m_fl_thr / m_single_thr\n",
        "    eff_pipe = speedup_pipe / n_stages_pipeline\n",
        "    eff_fl   = speedup_fl / n_clients_fl\n",
        "\n",
        "    print(f\"[Perf Summary]\")\n",
        "    print(f\"Single: time/epoch={m_single_t:.2f}s | thr={m_single_thr:.1f} samp/s | final acc={single_log['acc_curve'][-1]:.4f}\")\n",
        "    print(f\"Pipe:   time/epoch={m_pipe_t:.2f}s   | thr={m_pipe_thr:.1f} samp/s | final acc={pipe_log['acc_curve'][-1]:.4f} | bubbleâ‰ˆ{pipe_log['theoretical_bubble']:.3f}\")\n",
        "    print(f\"FL:     time/round={m_fl_t:.2f}s     | thr={m_fl_thr:.1f} samp/s | final acc={fl_log['acc_hist'][-1]:.4f} | final ASR={fl_log['asr_hist'][-1]:.4f}\")\n",
        "\n",
        "    print(f\"\\nSpeedup (vs single) â†’ Pipeline: {speedup_pipe:.2f} (eff={eff_pipe:.2f}); FL: {speedup_fl:.2f} (eff={eff_fl:.2f})\")\n",
        "\n",
        "    # Plots\n",
        "    fig, axs = plt.subplots(1,3, figsize=(18,4))\n",
        "    axs[0].bar([\"Single(epoch)\",\"Pipeline(epoch)\",\"FL(round)\"], [m_single_t, m_pipe_t, m_fl_t], color=[\"#4e79a7\",\"#59a14f\",\"#f28e2c\"])\n",
        "    axs[0].set_title(\"Time per epoch/round (s)\"); axs[0].grid(True, alpha=0.3)\n",
        "\n",
        "    axs[1].bar([\"Single\",\"Pipeline\",\"FL\"], [m_single_thr, m_pipe_thr, m_fl_thr], color=[\"#4e79a7\",\"#59a14f\",\"#f28e2c\"])\n",
        "    axs[1].set_title(\"Throughput (samples/sec)\"); axs[1].grid(True, alpha=0.3)\n",
        "\n",
        "    axs[2].bar([\"Pipeline\",\"FL\"], [speedup_pipe, speedup_fl], color=[\"#59a14f\",\"#f28e2c\"])\n",
        "    axs[2].axhline(1.0, color='gray', linestyle='--', linewidth=1)\n",
        "    axs[2].set_title(\"Speedup vs Single (â†‘ is better)\"); axs[2].grid(True, alpha=0.3)\n",
        "    plt.show()\n",
        "\n",
        "plot_perf(single_log, pipe_log, fl_log)"
      ],
      "metadata": {
        "id": "FzHraV76R-Ky"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Inference:**\n",
        "\n",
        "  - The plots show **Federated Learning (FL)** achieving the **lowest time per round** and highest throughput, giving ~1.0Ã— speedup vs singleâ€‘node (parity/slight gain).\n",
        "  - Pipeline parallelism under this setup is **slower per epoch** with lower throughput and ~0.86Ã— speedup, indicating pipeline/bubble overhead (especially on a single GPU without true overlap). Singleâ€‘node serves as a stable baseline between the two.\n",
        "  - **Takeaway:** For this experiment, **FL offers the best latency/throughput**; use pipeline primarily for model/activation memory scaling rather than raw speed.\n"
      ],
      "metadata": {
        "id": "3k9yeac8aDdu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "================================================================="
      ],
      "metadata": {
        "id": "EIX9RaFFaHOs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# B) Convergence & Final Accuracy"
      ],
      "metadata": {
        "id": "jjV3aowkSHEN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_acc(single_log, pipe_log, fl_log):\n",
        "    fig, axs = plt.subplots(1,3, figsize=(18,4))\n",
        "    axs[0].plot(range(1, len(single_log[\"acc_curve\"])+1), single_log[\"acc_curve\"], marker='o'); axs[0].set_title(\"Single-node Acc vs Epoch\")\n",
        "    axs[1].plot(range(1, len(pipe_log[\"acc_curve\"])+1), pipe_log[\"acc_curve\"], marker='o', color=\"#59a14f\"); axs[1].set_title(\"Pipeline Acc vs Epoch\")\n",
        "    axs[2].plot(range(1, len(fl_log[\"acc_hist\"])+1), fl_log[\"acc_hist\"], marker='o', color=\"#f28e2c\"); axs[2].set_title(\"FL Acc vs Round\")\n",
        "    for ax in axs:\n",
        "        ax.set_xlabel(\"Epoch/Round\"); ax.set_ylabel(\"Top-1 Acc\"); ax.grid(True, alpha=0.3)\n",
        "    plt.show()\n",
        "\n",
        "plot_acc(single_log, pipe_log, fl_log)"
      ],
      "metadata": {
        "id": "pnfzMk2oSMzx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Inference:**\n",
        "  - Single-node and pipeline training show steady accuracy improvement with epochs, with pipeline slightly outperforming singleâ€‘node due to better utilization.\n",
        "  - In contrast, federated learning (FL) shows very slow and unstable accuracy gains across rounds, reflecting communication overhead, nonâ€‘IID data, and aggregation noise. This highlights the accuracyâ€“efficiency tradeâ€‘off: centralized methods converge faster, **while FL prioritizes decentralization and trust over rapid convergence.**"
      ],
      "metadata": {
        "id": "ip8aVnyKbfok"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "========================================================================="
      ],
      "metadata": {
        "id": "zXeXY8KWdr8q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# C) Trustworthiness: ASR & Accuracy under Attack"
      ],
      "metadata": {
        "id": "PCRgEj9BSV60"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "#@title ðŸŽ¯ Backdoor ASR vs Rounds (FL)\n",
        "def plot_asr(fl_log):\n",
        "    rounds = range(1, len(fl_log[\"asr_hist\"])+1)\n",
        "    plt.figure(figsize=(5,4))\n",
        "    plt.plot(rounds, fl_log[\"asr_hist\"], marker='o', color=\"crimson\")\n",
        "    plt.title(\"Backdoor ASR vs Rounds (FL)\")\n",
        "    plt.xlabel(\"Round\")\n",
        "    plt.ylabel(\"ASR\")\n",
        "    plt.grid(True, alpha=0.3)\n",
        "    plt.show()\n",
        "\n",
        "plot_asr(fl_log)"
      ],
      "metadata": {
        "id": "oTSQcopjSZ3A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Inference:**\n",
        "\n",
        "The backdoor ASR remains near zero across rounds, with only a small spike in round 2, indicating limited attack success. This shows that robust aggregation and clipping effectively suppress backdoor influence. Overall, the FL system maintains strong robustness against backdoor attacks while training progresses."
      ],
      "metadata": {
        "id": "0ZmGqPMHb6e1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "==================================================================="
      ],
      "metadata": {
        "id": "TbTuK7cucC6i"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# D) Attack Detection: Gradient Mean vs Std"
      ],
      "metadata": {
        "id": "G6jlU25QSiYP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "#@title ðŸ§ª Gradient mean vs std scatter (round 1)\n",
        "gs = fl_log[\"grad_scatter_round\"]\n",
        "plt.figure(figsize=(5.5,4.2))\n",
        "for mean, std, mal in zip(gs[\"mean\"], gs[\"std\"], gs[\"is_mal\"]):\n",
        "    if mal: plt.scatter(mean, std, c=\"red\", label=\"Malicious\" if \"Malicious\" not in plt.gca().get_legend_handles_labels()[1] else \"\")\n",
        "    else:   plt.scatter(mean, std, c=\"blue\", label=\"Benign\" if \"Benign\" not in plt.gca().get_legend_handles_labels()[1] else \"\")\n",
        "plt.xlabel(\"Mean\")\n",
        "plt.ylabel(\"Std Dev\")\n",
        "plt.title(\"Client Gradient Stats (Round 1)\")\n",
        "plt.legend()\n",
        "plt.grid(True, alpha=0.3)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "o3dly4JpSm8e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Inference:**\n",
        "\n",
        "The gradient scatter clearly separates the malicious client (red outlier) from the tight benign cluster (blue), indicating strong detectability."
      ],
      "metadata": {
        "id": "nFpRnXDDcflA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "==========================================================================="
      ],
      "metadata": {
        "id": "r-XZtUI0ckkY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#E) Robust Aggregation Signals: Rejected Ratio & Gradient Variance"
      ],
      "metadata": {
        "id": "YjDbGjAKS4-F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "#@title ðŸ”Ž Rejected (clipped) ratio & Gradient variance\n",
        "rounds = range(1, len(fl_log[\"reject_ratio_hist\"])+1)\n",
        "fig, axs = plt.subplots(1,2, figsize=(12,4))\n",
        "axs[0].plot(rounds, np.array(fl_log[\"reject_ratio_hist\"])*100, marker='o'); axs[0].set_title(\"Clipped (proxy rejected) % vs Round\"); axs[0].set_ylabel(\"%\")\n",
        "axs[1].plot(rounds, fl_log[\"grad_var_hist\"], marker='o', color=\"purple\"); axs[1].set_title(\"Aggregated Gradient Variance vs Round\")\n",
        "for ax in axs:\n",
        "    ax.set_xlabel(\"Round\")\n",
        "    ax.grid(True, alpha=0.3)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "_MEDztG8S-Pt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Inference:**\n",
        "  - Clipped (proxyâ€‘rejected) = 0% across rounds â‡’ all client updates stayed within the â„“2 bound; current clip threshold is conservative.\n",
        "  - Aggregated gradient variance drops from ~3.1eâ€‘6 â†’ 1.4eâ€‘6, showing the robust aggregator is stabilizing training despite the outlier.\n",
        "  - Overall, the **FL defense is effective**; if you want stricter screening, lower the clip bound or add a cosineâ€‘similarity filter to proactively remove outliers."
      ],
      "metadata": {
        "id": "e47JzB6icnta"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "============================================================="
      ],
      "metadata": {
        "id": "AS8oE0vvc2H7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# F) Communication Cost per Round"
      ],
      "metadata": {
        "id": "vcjb8Ge1TGxL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "#@title ðŸŒ Communication cost per round (MB)\n",
        "plt.figure(figsize=(5,4))\n",
        "plt.plot(range(1, len(fl_log[\"comm_cost_mb_hist\"])+1), fl_log[\"comm_cost_mb_hist\"], marker='o', color=\"#e15759\")\n",
        "plt.title(\"Communication Cost per Round (MB)\")\n",
        "plt.xlabel(\"Round\")\n",
        "plt.ylabel(\"MB\")\n",
        "plt.grid(True, alpha=0.3)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "I-E8NHW4TLn0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Inference:**\n",
        "\n",
        "Communication cost per FL round is flat at ~6.6â€¯MB, indicating itâ€™s dominated by model/gradient size Ã— #clients and is independent of training progress or attacks."
      ],
      "metadata": {
        "id": "NXNVhqWSdIBC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "=========================================================="
      ],
      "metadata": {
        "id": "-ojZZZxqdHbm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Conclusion\n",
        "The TDML framework presents a significant advancement toward solving the limitations of existing distributed machine learning techniques. By integrating blockchain technology for reliability and security, it provides a scalable, efficient, and cost-effective solution for training large deep learning models in environments characterized by resource constraints and malice threats. The experimental evaluations underscore the framework's pragmatic capabilities and position it as a robust alternative to traditional DML architectures."
      ],
      "metadata": {
        "id": "RuI8iSe6dMKJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "======================== End ================================================"
      ],
      "metadata": {
        "id": "ZGBqrWgTdQzQ"
      }
    }
  ]
}